[{"content":" Puedes encontrar el código fuente del post en el siguiente enlace: https://github.com/fjvela/cosign-trivy-kyverno-in-action\nIntroducción Vamos a describir cómo podemos verificar la autenticidad e integridad de una imagen firmada en el proceso de integración continua y cómo verificar un attestation (información verificable sobre la imagen: revisión código fuente, escaneo de vulnerabilidades\u0026hellip;) con la información de las vulnerabilidades de la imagen antes de ejecutar la imagen en un cluster Kubernetes.\nPara ello vamos a utilizar las siguientes herramientas:\nCosign Kyverno Trivy Cosign Cosign es una herramienta creada por el proyecto Sigstore que nos permite firmar imágenes Docker y otros artefactos software.\nSu principal función es proporcionar una forma sencilla y segura de firmar digitalmente software, permitiendo verificar posteriormente su autenticidad e integridad.\nLas firmas se pueden almacenar junto con las imágenes en un registro de imágenes compatible con OCI (Open Container Initiative).\nPor defecto, Cosign firma los artefactos software utilizando el modo Keyless, para ello utiliza Fulcio como entidad certificadora y Rekor como transparency log. En caso de que fuera necesario también es posible realizar la firma utilizando nuestro certificado.\nUna attestation es una \u0026ldquo;declaración\u0026rdquo; firmada que contiene metadatos sobre la imagen Docker. Pueden incluir información sobre un escaneo de vulnerabilidades de la imagen o información de cómo se ha generado. Estas se almacenan en el registro de imágenes junto con la imagen y pueden ser verificadas por Cosign o por otras herramientas cómo Kyverno.\nTrivy Trivy es un escáner de seguridad que nos permite escanear diferentes elementos como:\nImágenes Ficheros Repositorios Git cluster de Kubernetes Dispone de distintos tipos de escáneres que podemos ejecutar en cualquier momento dentro de nuestros procesos de CI/CD:\nEscaneo de vulnerabilidades (CVEs) Paquetes y dependencias (SBOM) Configuraciones en el código de infraestructura (IaC) Información sensible (contraseñas y secretos) Kyverno Kyverno es gestor de políticas que nos va a permitir validar y/o modificar las peticiones de creación o modificación de los elementos de un cluster de Kubernetes. Para que esto sea posible, Kyverno se configura en el cluster como un Admission controller para recibir todas las peticiones realizadas a la API de Kubernetes.\nLas políticas se definen utilizando YAML, eliminando así la necesidad de aprender lenguajes específicos. Estas políticas permiten verificar si las imágenes han sido firmadas o si los elementos creados en el cluster cumplen con nuestros estándares de calidad y seguridad.\nGeneración de la imagen y firma Para poder continuar instala las siguientes herramientas:\nDocker Cosign Trivy Antes de comenzar con el proceso debes autenticarte en el almacén de imágenes que vayas a utilizar (si usas GitHub: https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)\nGeneramos nuestra imagen: docker build . -t ghcr.io/fjvela/blog-demo-image-sign-attestation:1.0.0 Almacenamos la imagen generada en nuestro contenedor de imágenes: docker push ghcr.io/fjvela/blog-demo-image-sign-attestation:1.0.0 Obtenemos el digest (el valor es único por cada imagen generada y nos aseguramos que estamos firmando la imagen que queremos) de la imagen generada: docker inspect --format='{{index .RepoDigests}}' ghcr.io/fjvela/blog-demo-image-sign-attestation:1.0.0 Firmamos la imagen con Cosign cosign sign ghcr.io/fjvela/blog-demo-image-sign-attestation@sha256:76e0af3bda5badd9b8e9772903bfdc8d5e2810f647ea6942e73d913a467c2cff. Cuando ejecutamos el comando cosign sign se ejecutan las siguientes acciones:\nSe verifica nuestra identidad a través de un Identity provider. Una vez obtenido un OIDC token, Cosign solicita un certificado (válido durante 10 minutos) a una entidad certificadora. En este caso a Fulcio, el cual emite certificados efímeros basados en identidades OIDC para firmar artefactos de software de forma segura. Se firma la imagen con el certificado, se genera un timestamp. A través de la información del timestamp podemos: Demostrar cuándo se creó la firma Ayuda a prevenir ataques de repetición Proporciona una línea temporal auditable de la firma del contenedor Permite decisiones de políticas basadas en tiempo en controladores de admisión como Kyverno Toda la información se almacena en nuestro container registry. La información sobre el certificado, la firma y el timestamp se envía a Rekor. Rekor es un registro inmutable y transparente (transparency log) que almacena y permite verificar firmas digitales y metadatos de artefactos de software. Creación informe de vulnerabilidades y attestation Una vez firmada la imagen, generamos un informe con las vulnerabilidades de la imagen que hemos generado. El siguiente comando nos proporciona el formato necesario para poder crear un attestation utilizando cosign:\ntrivy image --ignore-unfixed --format cosign-vuln --output vuln.json ghcr.io/fjvela/blog-demo-image-sign-attestation@sha256:76e0af3bda5badd9b8e9772903bfdc8d5e2810f647ea6942e73d913a467c2cff\nCreamos el attestation utilizando cosign:\ncosign attest --type vuln --predicate vuln.json ghcr.io/fjvela/blog-demo-image-sign-attestation@sha256:76e0af3bda5badd9b8e9772903bfdc8d5e2810f647ea6942e73d913a467c2cff\nPuedes consultar los tipos de attestation que puedes incluir en el siguiente enlace: https://docs.sigstore.dev/cosign/verifying/attestation/\nCon los siguientes comandos, se puede verificar la firma e información incluida en el attestation:\ncosign verify ghcr.io/fjvela/blog-demo-image-sign-attestation@sha256:76e0af3bda5badd9b8e9772903bfdc8d5e2810f647ea6942e73d913a467c2cff cosign verify-attestation --type vuln ghcr.io/fjvela/blog-demo-image-sign-attestation@sha256:76e0af3bda5badd9b8e9772903bfdc8d5e2810f647ea6942e73d913a467c2cff Verificación de firma y del escaneo de vulnerabilidades utilizando Kyverno En el siguiente enlace puedes consultar cómo instalar Kyverno en un cluster Kubernetes: https://kyverno.io/docs/installation/methods. Kyverno se despliega en el cluster cómo un Admission Controller, si lo vas a utilizar en producción es importante que revises la siguiente información: https://kyverno.io/docs/introduction/admission-controllers/\nKyverno soporta diferentes tipos de políticas. El tipo verifyImages nos permite realizar comprobaciones sobre las imágenes que se despliegan en nuestro cluster. También dispone de una amplia galería de políticas ya creadas y que nos pueden servir como base para crear nuestras propias políticas.\nPartiendo de la política Require Image Vulnerability Scans:\napiVersion: kyverno.io/v1 kind: ClusterPolicy metadata: name: require-vulnerability-scan spec: validationFailureAction: Audit webhookTimeoutSeconds: 10 failurePolicy: Fail rules: - name: scan-not-older-than-one-week match: any: - resources: kinds: - Pod verifyImages: - mutateDigest: false imageReferences: - \u0026#34;ghcr.io/fjvela/blog-demo-image-sign-attestation*\u0026#34; attestors: - entries: - keyless: subject: \u0026#34;fjvela@gmail.com\u0026#34; issuer: \u0026#34;https://github.com/login/oauth\u0026#34; rekor: url: https://rekor.sigstore.dev attestations: - type: https://cosign.sigstore.dev/attestation/vuln/v1 attestors: - entries: - keyless: subject: \u0026#34;fjvela@gmail.com\u0026#34; issuer: \u0026#34;https://github.com/login/oauth\u0026#34; rekor: url: https://rekor.sigstore.dev conditions: - all: - key: \u0026#34;{{ time_since(\u0026#39;\u0026#39;,\u0026#39;{{metadata.scanFinishedOn}}\u0026#39;,\u0026#39;\u0026#39;) }}\u0026#34; operator: LessThanOrEquals value: \u0026#34;1h\u0026#34; Conclusión Como has podido comprobar, la combinación de Cosign para la firma digital, Trivy para el análisis de vulnerabilidades y Kyverno para la gestión de políticas de seguridad en Kubernetes, nos proporciona una manera sencilla de comprobar la integridad y autenticidad de las imágenes que desplegamos en un cluster Kubernetes.\nLa combinación de estas herramientas nos permite:\nAsegurar la integridad y autenticidad de nuestras imágenes mediante firmas digitales Detectar vulnerabilidades de seguridad antes del despliegue Automatizar la verificación de seguridad durante el despliegue en Kubernetes Mantener un registro inmutable de todas las firmas y attestations Referencias https://docs.sigstore.dev/cosign/signing/signing_with_containers/ https://docs.sigstore.dev/logging/overview/ https://github.com/sigstore/fulcio https://github.com/sigstore/rekor https://kyverno.io/ https://trivy.dev/latest/ ","permalink":"https://blog.javivela.dev/posts/2025/k8s/cosign-trivy-kyverno-en-accion/","summary":"En el artículo explico como utilizar Cosign para firmar imágenes Docker mediante Keyless signing, generar información de vulnerabilidades con Trivy, atestar la información con Cosign y, finalmente, mostrar como Kyverno verifica la integridad de la imagen y sus metadatos antes de ejecutarla en un cluster de Kubernetes.","title":"Cosign, Trivy y Kyverno en acción"},{"content":" Puedes encontrar el código fuente del post en el siguiente enlace: https://github.com/fjvela/blog-demo-azure-ad-workload-identity\nIntroducción En el siguiente post voy a explicar cómo puedes utilizar Azure Managed Identities y Azure Federated Identity Credentials desde un cluster Kubernetes desplegado en Azure. Esto permitirá a las aplicaciones que se ejecutan en el cluster acceder a servicisos desplegados en Azure sin necesidad de utilizar credenciales.\nAntes de continuar leyendo el post, te recomiendo que leas los posts sobre Azure Managed Identities y Azure Federated Identity Credentials.\nEn los posts mencionados, explico cómo acceder a los recursos desplegados en Azure sin necesidad de gestionar secretos y/o certificados:\nAzure Managed Identities: Se usan en aplicaciones que se ejecutan en servicios de Azure (ej. máquinas virtuales). Azure Federated Identity Credentials: Se usan en aplicaciones y servicios que se ejecutan fuera de Azure (ej. GitHub Workflow). ¿Cómo funciona? Una federated identity credential permite establecer una relación de confianza entre una User managed identity y un proveedor de identidad externo (external identity provider IdP).\nEsta relación permite a Microsoft Identity autenticar una aplicación externa a través del token recibido y el IdP configurado, generando a su vez otro token que permitirá a la aplicación acceder a los recursos desplegados en Azure.\nTras crear la user managed identity, podemos configurar hasta un máximo de 20 federated identity credentials.\nPara aplicaciones desplegadas en un cluster de Kubernetes, se requiere la siguiente información:\nissuer: Dirección URL del proveedor de identidades externo del cluster (IdP). namespace: Nombre del namespace donde se ejecutarán los pods. service account: Nombre del service account (SA) asociado a los pods de nuestro workload. audiences: Indica qué plataforma de identidad de Microsoft debe aceptar el token entrante (por defecto: api://AzureADTokenExchange - el valor depende del entorno cloud utilizado - Fairfax, Mooncake, USNat o USSec). Nota: La combinación de namespace y service account debe ser única en la user managed identity.\nCuando Kubernetes inicia un nuevo POD, AD Workload Identity crea un token en un path determinado. La aplicación solicita un token a Microsoft identity platform, utilizando el token generado en el paso 1. Microsoft identity platform valida el token incluido en la petición contra el servicio de validación externo que se ha registrado previamente (OIDC del cluster). Si el token es válido, se genera un token de autenticación para autenticarse contra otros servicios desplegados en Azure (hay que asignar los permisos necesarios en el servicio). La aplicación utiliza el token generado para acceder a los recursos desplegados en Azure. Configurar federated identity credentials (Kubernetes) Para comenzar, necesitamos un proveedor de identidad externo para crear la relación de confianza entre nuestro cluster y las managed identities utilizadas por las aplicaciones que se ejecutan en nuestro cluster.\nAzure nos proporciona el servicio OpenID Connect (OIDC). Para activarlo podemos usar el siguiente comando:\naz aks update --resource-group myResourceGroup --name myAKScluster --enable-oidc-issuer Si estás utilizando Amazon Elastic Kubernetes (EKS) o Google Kubernetes Engine (GKE) revisa la siguiente documentación:\nhttps://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html https://cloud.google.com/kubernetes-engine/enterprise/identity/setup/per-cluster A continuación, vamos a configurar una federated identity credential para permitir que la aplicación pueda acceder a recursos desplegados en Azure.\nDesde el menú \u0026ldquo;Managed Identities\u0026rdquo; podemos crear una User Managed Identity.\nUna vez creada, añadimos una federated identity credential:\nAzure nos da la opción de configuración para diferentes escenarios, en nuestro caso seleccionamos la opción para configurar Kubernetes. Completamos la información que nos solicita: Namespace donde se ejecuta la aplicación. Service Account (SA) que utiliza la aplicación. URL del emisor de OIDC de nuestro cluster, puedes consultarla en el portal o través del comando: az aks show --name myAKScluster --resource-group myResourceGroup --query \u0026quot;oidcIssuerProfile.issuerUrl\u0026quot; -o tsv. Después de crear la user managed identity y configurados los federated identity credentials, debemos asignar los permisos correspondientes de acceso.\nComo acceder a un recurso en Azure utilizando una Managed Identity utilizando .NET Podemos hacer uso de las Managed Identities creadas en Azure a través de los diferentes SDKs disponibles.\nVamos a describir cómo podemos acceder a un Azure Key Vault utilizando código .NET y la librería Azure.Identity:\nCrea un proyecto .NET, en este caso hemos creado un proyecto tipo Web. Añade la referencia a la librería Azure.Identity dotnet add package Azure.Identity. Azure.Identity proporciona varias clases que encapsulan la lógica necesaria para poder obtener un token de autenticación de Microsoft Entra ID haciendo uso de la Managed Identity configurada:\nDefaultAzureCredential: Intenta realizar la autenticación a través de diferentes mecanismos hasta que consigue conectarse con uno de ellos. var credential = new DefaultAzureCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); ChainedTokenCredential: Intenta realizar la autenticación a través de la Managed Identity. Si no lo consigue, intenta conectarse a través de Azure CLI\nvar credential = new ChainedTokenCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); ManagedIdentityCredential: Utiliza la Managed Identity para solicitar el token de autenticación\nvar credential = new ManagedIdentityCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); El código final de nuestro API controlador es el siguiente:\n[ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class SecretController : ControllerBase { private readonly ILogger\u0026lt;SecretController\u0026gt; _logger; private readonly IConfiguration _config; public SecretController(ILogger\u0026lt;SecretController\u0026gt; logger, IConfiguration config) { _logger = logger; _config = config; } [HttpGet(Name = \u0026#34;GetSecret\u0026#34;)] public IActionResult Get(string name, string credentialType = \u0026#34;DefaultAzureCredential\u0026#34;) { var kvName = _config[\u0026#34;KV_NAME\u0026#34;]; _logger.LogInformation($\u0026#34;C# HTTP trigger function processed a request. {name} {credentialType} {kvName}\u0026#34;); var tokenCredential = GetTokenCredential(credentialType); var client = new SecretClient(new Uri($\u0026#34;https://{kvName}.vault.azure.net/\u0026#34;), tokenCredential); return new OkObjectResult(client.GetSecret(name)); } private TokenCredential GetTokenCredential(string credentialType) { switch (credentialType) { case \u0026#34;DefaultAzureCredential\u0026#34;: return new DefaultAzureCredential(); case \u0026#34;ChainedTokenCredential\u0026#34;: return new ChainedTokenCredential(); case \u0026#34;ManagedIdentityCredential\u0026#34;: return new ManagedIdentityCredential(); } throw new Exception($\u0026#34;The credential type {credentialType} is not valid\u0026#34;); } } Como configurar Managed Identity en el cluster Para configurar la Managed Identity en el cluster es necesario crear y asignar una Service Account (SA) a nuestra aplicación.\nUtilizando las anotaciones azure.workload.identity/client-id y azure.workload.identity/tenant-id podemos configurar la Managed Identity creada previamente y que utilizaremos con nuestra aplicación. A continuación puedes ver un ejemplo:\napiVersion: v1 kind: ServiceAccount metadata: name: sa-app-1 namespace: develop annotations: azure.workload.identity/client-id: \u0026#34;3d49c3ea-369c-4af2-b770-3a5de4d0aca2\u0026#34; azure.workload.identity/tenant-id: \u0026#34;xxxxxx-xxx-x-x-x\u0026#34; Además es necesario añadir la etiqueta azure.workload.identity/use a los pods de nuestra aplicación para indicarle a Azure AD Workload Identity que debe crear un token de autenticación en el POD utilizando los datos de configuración proporcionados en la Service Account (SA).\nA continuación puedes encontrar el código yaml necesario para desplegar la aplicación en el cluster kubernetes haciendo uso de una Managed Identity creada previamente:\napiVersion: v1 kind: Namespace metadata: name: develop --- apiVersion: v1 kind: ServiceAccount metadata: name: sa-app-1 namespace: develop annotations: azure.workload.identity/client-id: \u0026#34;3d49c3ea-369c-4af2-b770-3a5de4d0aca2\u0026#34; azure.workload.identity/tenant-id: \u0026#34;xxxxxx-xxx-x-x-x\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: labels: app: myapp name: myapp namespace: develop spec: replicas: 1 selector: matchLabels: app: myapp strategy: {} template: metadata: labels: app: myapp azure.workload.identity/use: \u0026#34;true\u0026#34; spec: serviceAccountName: sa-app-1 containers: - image: acrqi99.azurecr.io/app:1.0.0 name: app ports: - containerPort: 8080 env: - name: KV_NAME value: kv-qi99 resources: {} Una vez desplegada la aplicación podemos comprobar como Azure AD Workload Identity ha modificado la configuración del POD para que la aplicación pueda acceder a los servicios desplegados en Azure haciendo uso de la Managed Identity configurada:\nRealizando una llamada GET para consultar la información del secreto secret-sauce, podemos comprobar que la Managed Identity ha sido configurada correctamente:\nConclusión La implementación de Azure Managed Identities junto con Azure Federated Identity Credentials y Azure AD Workload Identity proporciona una solución robusta y segura para gestionar el acceso a servicios de Azure a aplicaciones que se ejecutan en un cluster de Kubernetes.\nEsta solución elimina la necesidad de gestionar credenciales manualmente, reduciendo significativamente los riesgos de seguridad asociados con el manejo de secretos y certificados.\nLas ventajas principales de esta solución son:\nEliminar la gestión manual de credenciales. Mayor seguridad al no almacenar secretos en el código o en el cluster. Escalabilidad para múltiples aplicaciones y servicios. Compatibilidad con múltiples entornos y proveedores de Kubernetes. Referencias https://azure.github.io/azure-workload-identity/docs/ https://learn.microsoft.com/en-us/azure/aks/aks/use-oidc-issuer https://docs.aws.amazon.com/eks/latest/userguide/enable-iam-roles-for-service-accounts.html https://cloud.google.com/kubernetes-engine/enterprise/identity/setup/per-cluster ","permalink":"https://blog.javivela.dev/posts/2024/azure/aks/ad-workload-identity/","summary":"Azure Managed Identities, Federated Identity Credentials y Azure AD Workload Identity permiten el acceso seguro a servicios Azure desde Kubernetes sin gestionar credenciales, utilizando tokens de autenticación.","title":"Azure AD Workload Identity"},{"content":" Puedes encontrar el código fuente del post en el siguiente enlace: https://github.com/fjvela/blog-demo-federated-identity-credential\nIntroducción En el post Azure Managed Identities revisamos cómo funcionan y cómo podemos utilizar Managed Identities. Estas nos permiten autenticarnos contra servicios de Azure u otras aplicaciones desplegadas en Azure sin necesidad de gestionar secretos y/o certificados.\nExisten otros escenarios donde las aplicaciones o servicios desplegados fuera de Azure necesitan acceder a recursos desplegados en Azure.\nRotar y/o guardar secretos o certificados puede ser tedioso, además supone un riesgo para la seguridad ya que cualquiera que disponga de esta información podría acceder a estos servicios.\nLas federated identity credentials nos van a permitir autenticar aplicaciones y servicios externos sin necesidad de gestionar secretos o certificados.\n¿Cómo funciona? Una federated identity credential permite establecer una relación de confianza entre una User managed identity y un proveedor de identidad externo (external identity provider IdP).\nEsta relación va a permitir a Microsoft Identity autenticar a una aplicación externa a través del token recibido y el IdP configurado, generando a su vez otro token que permitirá a la aplicación acceder a los recursos desplegados en Azure.\nUna vez creada la user managed identity, podemos configurar hasta un máximo de 20 federated identity credentials.\nLa información necesaria es la siguiente:\nissuer: Dirección URL del proveedor de identidades externo (IdP). subject: Es el identificador de la aplicación externa, el formato depende del proveedor de identidad externo (IdP). audiences: Indica qué plataforma de identidad de Microsoft debe aceptar el token entrante (por defecto: api://AzureADTokenExchange - el valor depende del entorno cloud utilizado - Fairfax, Mooncake, USNat o USSec). La combinación de issuer y subject debe ser única en la user managed identity.\nLa aplicación solicita un token a su proveedor de identidades. La aplicación solicita un token a Microsoft identity platform, utilizando el token generado en el paso 1. Microsoft identity platform valida el token incluido en la petición contra el servicio de validación definido externo registrado. Si el token es válido, se genera un token de autenticación para autenticarse contra otros servicios desplegados en Azure (hay que asignar los permisos necesarios en el servicio). La aplicación utiliza el token generado para acceder a los recursos desplegados en Azure. Configurar federated identity credentials (GitHub) A continuación, vamos a configurar una federated identity credential para permitir que un GitHub workflow pueda acceder a recursos desplegados en Azure.\nDesde el menú \u0026ldquo;Managed Identities\u0026rdquo; podemos crear una User Managed Identity.\nUna vez creada, añadimos una federated identity credential:\nAzure nos da la opción de configuración para diferentes escenarios, en nuestro caso seleccionamos la opción para configurar GitHub. Completamos la información de la organización, repositorio y entidad (entity). En nuestro caso seleccionamos la entidad \u0026ldquo;branch\u0026rdquo; e introducimos el nombre del branch que estará autorizado a utilizar los recursos de Azure. No se pueden utilizar wildcards (*). Si necesitamos dar permisos a otros branches, debemos añadir cada uno de ellos. Una vez creada la user managed identity y configurados los federated identity credentials, debemos asignar los permisos correspondientes de acceso.\nCrear GitHub workflow Para comprobar el correcto funcionamiento de la federated identity credential vamos a crear un GitHub workflow que realiza un login y mostrará la información de la suscripción de Azure.\nPara ello vamos a utilizar el GitHub action Azure login:\nname: Run Azure Login with OIDC on: [push] permissions: id-token: write contents: read jobs: azure-login: runs-on: ubuntu-latest steps: - name: Azure login uses: azure/login@v2 with: client-id: ${{ secrets.AZURE_CLIENT_ID }} tenant-id: ${{ secrets.AZURE_TENANT_ID }} subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }} - name: Azure CLI script uses: azure/cli@v2 with: azcliversion: latest inlineScript: | az account show Una vez que el contenido está en el repositorio, la GitHub action se ejecuta cada vez que se añade algo al branch main: Conclusión El uso de federated identity credentials representa un avance significativo en la administración de seguridad y autenticación para aplicaciones y servicios que interactúan con recursos de Azure desde entornos externos.\nAlgunas de las ventajas de su uso son:\nSeguridad mejorada: No se necesita administrar ni almacenar secretos y certificados, lo que reduce en gran medida el riesgo de que se comprometan las credenciales. Administración simplificada: La configuración de federated identity credentials es un proceso simple que reduce la carga administrativa asociada con la rotación y el mantenimiento de secretos. Flexibilidad: Permite una integración perfecta con una variedad de proveedores de identidad externos y se adapta a diferentes arquitecturas y requisitos de seguridad. Para los desarrolladores y administradores de sistemas que utilizan Azure, la adopción de esta tecnología puede mejorar significativamente la seguridad de las aplicaciones y al mismo tiempo simplificar el proceso de autenticación y autorización.\nReferencias https://learn.microsoft.com/en-gb/entra/identity/managed-identities-azure-resources/ https://learn.microsoft.com/en-us/entra/workload-id/ https://learn.microsoft.com/en-us/azure/developer/github/connect-from-azure?tabs=azure-portal%2Cwindows#use-the-azure-login-action-with-openid-connect https://learn.microsoft.com/en-us/azure/architecture/patterns/federated-identity https://github.com/Azure/login ","permalink":"https://blog.javivela.dev/posts/2024/azure/federated-identity-credential/","summary":"Azure Federated Identity Credentials simplifican la gestión de credenciales y la autenticación para aplicaciones que no se ejecutan en Azure. Las aplicaciones pueden acceder de forma segura a otros recursos de Azure sin necesidad de gestionar credenciales de forma manual.","title":"Azure Federated Identity Credentials"},{"content":" Puedes encontrar el código fuente del post en el siguiente enlace: https://github.com/fjvela/blog-managed-identities\nIntroducción Uno de los grandes retos que tenemos hoy en día es gestionar la información sensible (credenciales, certificados, \u0026hellip;) que nuestras aplicaciones necesitan.\nAzure proporciona Managed Identities para simplificar la autenticación necesaria para acceder a los recursos desplegados en Azure como Azure Key Vault. Puedes comprobar la lista de servicios que soportan la autenticación a través de Managed Identities en el siguiente enlace: https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/managed-identities-status\nLa utilización de Managed Identities tiene algunas ventajas:\nNo es necesario utilizar y/o gestionar credenciales Permite autenticarnos con cualquier otra aplicación (incluyendo nuestras propias aplicaciones) No tiene un coste extra Tipos de Managed Identities Existen dos tipos de Managed Identities:\nSystem managed identities: Son creadas automáticamente por Azure cuando se crean algunos servicios como máquinas virtuales: Solo se puede utilizar por el servicio que la ha creado Su ciclo de vida está asociado al ciclo de vida del recurso que lo ha creado (si borras el recurso, se borra la Managed Identity) Automáticamente se crea un Service Principal en Microsoft Entra ID (directorio activo) Debemos autorizar a qué servicios pueden acceder a la Managed Identity User managed identities: Podemos crear un Managed Identity y asignársela a más de una aplicación: Podemos utilizarla en una o varias aplicaciones Debemos autorizar a qué servicios pueden acceder las aplicaciones que usen la Managed Identity creada Automáticamente se crea un Service Principal en Microsoft Entra ID (directorio activo), su ciclo de vida no está asociado al ciclo de vida de la aplicación que lo utiliza ¿Cómo funciona? Antes de explicar como funciona el proceso de creación y funcionamiento de una managed identity, debemos conocer qué es Azure Instance Metadata Service identity (IMSI):\nEs un servicio REST interno disponible en la dirección IP: 169.254.169.254 Proporciona información sobre las instancias de VM en ejecución: Sistema operativo, mantenimientos programados, \u0026hellip; También proporciona tokens de autenticación que obtiene de Microsoft Entra ID Azure Resource Manager crea un service principal en Microsoft Entra ID asociado a la Managed Identity creada A continuación Azure Resource Manager, actualiza Azure Instance Metadata Service identity (IMSI) proporcionando el ID el service principal y los certificados creados por Microsoft Entra ID. Más tarde serán necesarios para poder autenticarse contra Microsoft Entra ID En este paso, Azure Resource Manager habilita los permisos necesarios para acceder a otros recursos de Azure (Azure RBAC). Por ejemplo, asignar el rol Key Vault Secrets User para poder leer secretos de un Azure Key Vault Cuando nuestra aplicación necesita acceder al recurso, se comunica con el IMSI, el cual solicita un token de autenticación a Microsoft Entra ID (utilizando el service principal y certificados creados en el paso 2) que será utilizado por la aplicación para poder acceder a otros recursos desplegados en Azure. Habilitar una Managed Identity en Azure Como hemos comentado, exiten dos tipos de Managed Identites: System managed identities y User managed identities. Vamos a ver como crear y habilitarlas en Azure.\nUna vez creada la managed identity, debemos asignar los permisos correspondientes de acceso.\nSystem Managed Identity Para crear un System Managed Identity asociado a un recurso de Azure tan solo debemos seleccionarlo desde el portal y en el menu \u0026ldquo;Identity\u0026rdquo;, habilitarlo y guardar los cambios realizados.\nUser Managed Identity Desde el apartado \u0026ldquo;Managed Identities\u0026rdquo; podemos crear User Managed Identites, una vez completados los datos requeridos debemos asignar la managed identity al recurso o recursos que harán uso de ella.\nComo acceder a un recurso en Azure utilizando una Managed Identity utilizando .NET Podemos hacer uso de las Managed Identities creadas en Azure a través de los diferentes SDKs disponibles o implementando el código necesario para poder interactuar con el API REST proporcionado por la IMSI y así obtener los tokens de autenticación para acceder a otros recursos de Azure.\nVamos a describir cómo podemos acceder a un Azure Key Vault utilizando código .NET y la librería Azure.Identity:\nCrea una proyecto .NET, en este caso hemos creado un proyecto tipo Azure Functions para facilitar su despliegue en Azure Añade la referencia a la librería Azure.Identity dotnet add package Azure.Identity Azure.Identity proporciona varias clases que encapsulan la lógica necesaria para poder obtener un token de autenticación de Microsoft Entra ID haciendo uso de la Managed Identity configurada a través de la IMSI:\nDefaultAzureCredential: Intenta realizar la autenticación a través de diferentes mecanismos hasta que consigue conectarse con uno de ellos. var credential = new DefaultAzureCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); ChainedTokenCredential: Intenta realizar la autenticación a través de la Managed Identity. Si no lo consigue, intenta conectarse utilizando a través de Azure CLI\nvar credential = new ChainedTokenCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); ManagedIdentityCredential: Utiliza la Managed Identity para solicitar el token de autenticación\nvar credential = new ManagedIdentityCredential(); var client = new SecretClient(new Uri(\u0026#34;https://mykv.vault.azure.net/\u0026#34;), credential); El código final de nuestra Azure Function es el siguiente:\n[Function(\u0026#34;GetSecret\u0026#34;)] public IActionResult GetSecret([HttpTrigger(AuthorizationLevel.Function, \u0026#34;get\u0026#34;)] HttpRequest req, [FromQuery] string name, [FromQuery] string credentialType) { _logger.LogInformation(\u0026#34;C# HTTP trigger function processed a request.\u0026#34;); var kvName = _config[\u0026#34;KV_NAME\u0026#34;]; var tokenCredential = GetTokenCredential(credentialType); var client = new SecretClient(new Uri($\u0026#34;https://{kvName}.vault.azure.net/\u0026#34;), tokenCredential); return new OkObjectResult(client.GetSecret(name)); } private TokenCredential GetTokenCredential(string credentialType) { switch (credentialType) { case \u0026#34;DefaultAzureCredential\u0026#34;: return new DefaultAzureCredential(); case \u0026#34;ChainedTokenCredential\u0026#34;: return new ChainedTokenCredential(); case \u0026#34;ManagedIdentityCredential\u0026#34;: return new ManagedIdentityCredential(); } throw new Exception($\u0026#34;The credential type {credentialType} is not valid\u0026#34;); } A continuación podemos ver el resultado de la ejecución de la Azure Function desde local y desde Azure:\nConclusión La utilización de Managed Identities no solo simplifica la gestión de credenciales, sino que también mejora significativamente la seguridad de las aplicaciones, al eliminar la necesidad de configurar y guardar secretos en archivos de configuración.\nPara los desarrolladores y arquitectos, utilizar Managed Identities debería ser una práctica estándar en la mayoría de los escenarios.\nReferencias https://learn.microsoft.com/en-us/training/modules/implement-managed-identities/5-acquire-access-token https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview https://learn.microsoft.com/en-us/entra/identity-platform/how-applications-are-added https://learn.microsoft.com/en-us/azure/postgresql/single-server/how-to-connect-with-managed-identity https://learn.microsoft.com/en-us/dotnet/api/overview/azure/identity-readme https://learn.microsoft.com/es-es/azure/virtual-machines/instance-metadata-service ","permalink":"https://blog.javivela.dev/posts/2024/azure/managed-identities/","summary":"Managed Identities simplifican la gestión de credenciales y la autenticación en Azure. Las aplicaciones pueden acceder de forma segura a otros recursos de Azure sin necesidad de gestionar credenciales de forma manual.","title":"Azure Managed Identities"},{"content":" Post actualizado a .NET Aspire 8.2\nIntroducción Actualmente, gran parte de las aplicaciones que desarrollamos son distribuidas o están diseñadas para desplegarse en la nube.\nEl desarrollo de este tipo de aplicaciones presenta varios retos. Entre ellos se encuentran la configuración de los entornos de desarrollo, la integración y utilización de servicios en la nube y la tarea de desplegar y orquestar la solución en proveedores como Azure o Amazon Web Services.\nPara simplificar este proceso, Microsoft ha creado .NET Aspire, un framework que agiliza el desarrollo de aplicaciones distribuidas, observables y listas para ser desplegadas en producción. Los desarrolladores pueden concentrarse en la lógica de negocio, mientras .NET Aspire se encarga de tareas como la orquestación de servicios y su configuración.\nOrquestación (App Host) .NET Aspire nos ofrece un conjunto de librerías para poder configurar dependencias de servicios internos y externos de nuestras aplicaciones. Una vez definidas estas dependencias, .NET Aspire se encarga de configurar y/o crear los servicios necesarios para que la aplicación pueda funcionar correctamente.\nEs importante destacar que .NET Aspire no pretende sustituir a otros sistemas de orquestación de servicios como Kubernetes.\nPara poder hacer uso de la orquestación de .NET Aspire necesitamos crear un proyecto AppHost. En el proyecto podemos definir nuestras aplicaciones, servicios, las dependencias entre ellos para una correcta ejecución de la solución. A continuación podemos ver un ejemplo:\nvar builder = DistributedApplication.CreateBuilder(args); var cache = builder.AddRedis(\u0026#34;cache\u0026#34;) .WithRedisCommander(); var api = builder.AddProject\u0026lt;Api\u0026gt;(\u0026#34;api\u0026#34;) .WithReference(cache); var web = builder.AddProject\u0026lt;MyWeatherHub\u0026gt;(\u0026#34;myweatherhub\u0026#34;) .WithReference(api) .WithExternalHttpEndpoints(); builder.Build().Run(); En el ejemplo y diagrama anteriores podemos ver la definición y dependencias de una solución con tres componentes: caché, API y frontend definido como myweatherhub. Cuando iniciamos la solución, .NET Aspire se encarga de establecer los parámetros de configuración necesarios como Connection Strings o Service Bindings, ejecutar cada una de las aplicaciones y servicios en el orden necesario para que el conjunto de la solución arranque y funcione correctamente.\nPara definir y configurar los componentes de la solución, .NET Aspire nos ofrece varios tipos recursos:\nProjectResource: Define la dependencia de un projecto .NET, como ASP.NET Core. ContainerResource: Dependencia con una imagen Docker. ExecutableResource: Dependencias con ficheros ejecutables que se deben ejecutar como parte de la solución. Es importante comentar que .NET Aspire no se limita unicamente a proyectos .NET. Tambien es posible incluir aplicaciones de otras tecnologias como Node.js (Angular, React o Vue) o Python. Podeis consultar mas información en los siguientes enlaces:\nhttps://learn.microsoft.com/en-us/dotnet/aspire/get-started/build-aspire-apps-with-nodejs https://learn.microsoft.com/en-us/dotnet/aspire/get-started/build-aspire-apps-with-python?tabs=bash Service defaults El tipo de proyecto .NET Aspire service defaults nos permite aplicar una configuración común a todos los proyectos de una solución. Simplificando la gestión de elementos como:\nTelemetría (OpenTelemetry): Utilizando OpenTelemetry para recolectar y exportar datos de rastreo y métricas. Health checks: Monitorización del estado de la aplicación Service Discovery: Habilitar el servicio ServiceDiscovery (invocación de aplicaciones y servicios por su nombre lógico) Resilience: Implementación de patrones para mejorar la tolerancia a fallos Al centralizar las configuraciones en un único lugar, ahorramos tiempo y garantizamos una mayor coherencia en toda la solución.\nIntegrations .NET Aspire integrations facilitan la integración de servicios y plataformas como Apache Kafka, Azure Service Bus o PostgreSQL en cualquier aplicación .NET. Sin necesidad de una configuración compleja, estas librerías se integran de forma sencilla, permitiendo conectar con una amplia variedad de servicios externos.\nPodemos hacer uso de .NET Aspire integrations sin necesidad de añadir un proyecto App Host o Service Defaults a la solución.\nPara ello:\nAñadir la referencia del paquete NuGet: dotnet add Aspire.StackExchange.Redis.OutputCaching Registrar el componente como servicio de la aplicación: builder.AddRedisOutputCache(\u0026quot;cache\u0026quot;); La configuración de los componentes se puede realizar a través del fichero appsettings.json o utilizando código .NET:\n{ \u0026#34;Aspire\u0026#34;: { \u0026#34;StackExchange\u0026#34;: { \u0026#34;Redis\u0026#34;: { \u0026#34;ConfigurationOptions\u0026#34;: { \u0026#34;ConnectTimeout\u0026#34;: 3000, \u0026#34;ConnectRetry\u0026#34;: 2 }, \u0026#34;DisableHealthChecks\u0026#34;: true, \u0026#34;DisableTracing\u0026#34;: false } } } } builder.AddRedisOutputCache( \u0026#34;cache\u0026#34;, static settings =\u0026gt; settings.DisableHealthChecks = true); Por defecto:\nProporcionan información sobre su estado de salud (health checks): Esto permite detectar y solucionar problemas de forma proactiva. Implementan mecanismos de resiliencia (resiliency): Como reintentos y timeouts, para garantizar la disponibilidad de los servicios. Ofrecen observabilidad y telemetría (telemetry): Facilitando la monitorización y el diagnóstico de problemas. Dashboard Cuando integramos el framework .NET Aspire en la solución tenemos acceso a un dashboard cuando ejecutamos la solución. Este dashboard nos va a proporcionar la siguiente información:\nLogs (console logs): Muestra los logs que escriben nuestras aplicaciones en consola. Logs estructurados (structured logs): Muestra eventos generados por OpenTelemetry: recurso relacionado, fecha, hora, nivel de log, mensaje\u0026hellip; Trazas (traces): Podemos consultar el flujo de ejecución de cada una de las peticiones que ha recibido la aplicación para detectar posibles cuellos de botella y errores. Métricas (metrics): Esta página muestra las gráficas y datos relacionados con las diferentes métricas (CPU, memoria, peticiones) que genera la aplicación. Despliegue .NET Aspire no solo nos ayuda a desarrollar nuestra solución distribuida, sino que también ofrece diversas herramientas para desplegar nuestra solución en Azure, ya sea desde nuestro entorno de desarrollo o mediante un sistema de CI/CD.\nAzure Container Apps Azure Container Apps es una plataforma serverless auto gestionada por Azure. Permite ejecutar aplicaciones contenerizadas sin necesidad de preocuparnos de su configuración o su mantenimiento, ya que Azure se encarga de estas tareas.\nPara desplegar la solución en Azure Container Apps utilizamos la linea de comandos azd (Azure Developer CLI). Una vez instalada, ejecutamos los siguientes comandos desde la carpeta de nuestro proyecto AppHost:\nazd init: inicializa y configura la solución para ser desplegada. azd up: analiza, genera los fichero con la infrastructura necesaria (bicep), compila y publica las images Docker de las aplicaciones y por último realiza el despliegue completo de la solucion. Tras el despliegue, podemos acceder tanto a la aplicación como al dashboard de .NET Aspire.\n\u0026#x26a0;\u0026#xfe0f; Utiliza el comando azd down para eliminar toda la infrastructura desplegada en Azure.\nKubernetes A través del proyecto open source Aspir8 generamos los recursos necesarios para desplegar la solución en un clúster Kubernetes.\nUn vez instalada la herramienta Aspir8, ejecutamos los siguientes comandos desde la carpeta de nuestro proyecto AppHost:\naspirate init: inicializa el proyecto aspirate build: genera los recusos necesario para desplegar la solucion en un clúster Kubernetes en diferentes formatos como Helm Chart o kustomize. \u0026#x26a0;\u0026#xfe0f; Los pasos anteriores no han generado y ni publicado las imagenes Docker de la solucion, debes generar y publicarlas manualmente o debes configurar un sistema de CI/CD en tu repositorio GIT.\nReferencias https://learn.microsoft.com/en-gb/training/paths/dotnet-aspire/ https://learn.microsoft.com/en-us/dotnet/aspire/ https://youtu.be/NAYeP0KhLZI?si=VhJ9Ob3Jgs7QxE2v\u0026t=2 https://www.youtube.com/live/dd1Mc5bQZSo?si=TESCzmMwZrdbRb-J\u0026t=108 https://github.com/dotnet/aspire https://github.com/dotnet/tye https://prom3theu5.github.io/aspirational-manifests/getting-started.html https://github.com/fjvela/letslearn-dotnet-aspire ","permalink":"https://blog.javivela.dev/posts/2024/dotnet/desarrolla-aplicaciones-distribuida-dotnet-aspire/","summary":".NET Aspire un framework que agiliza el desarrollo de aplicaciones distribuidas, observables y listas para ser desplegadas en producción.","title":"Desarrolla aplicaciones distribuidas con .NET Aspire"},{"content":"Ya va quedando menos para el lanzamiento de .NET 9 y tenemos disposibles algunas de las novedades que incluirá C# 13 en su lanzamiento A continuación revisamos algunas de ellas.\nPuedes encontrar el código fuente de los ejemplos en repositorio: https://github.com/fjvela/csharp-13\nparams collections El modificador params nos permite pasar un número variable de argumentos a un método. Hasta el momento estaba limitado unicamente al uso con tipos de datos Array.\nvar persons = new List\u0026lt;Person\u0026gt; { new Person(\u0026#34;Mads\u0026#34;, \u0026#34;Torgersen\u0026#34;), new Person(\u0026#34;Dustin\u0026#34;, \u0026#34;Campbell\u0026#34;), new Person(\u0026#34;Kathleen\u0026#34;, \u0026#34;Dollard\u0026#34;) }; static void WriteNames(params string[] names) =\u0026gt; Console.WriteLine(String.Join(\u0026#34;, \u0026#34;, names)); WriteNames(persons.Select(person =\u0026gt; person.FirstName).ToArray()); internal class Person( string Name, string FirstName) { public string Name { get; set; } public string FirstName { get; set; } } C# 13 extiende el modificador permitiendonos trabajar con cualquier tipo de colección, tales como: System.Span\u0026lt;T\u0026gt;_, _System.ReadOnlySpan\u0026lt;T\u0026gt;, y tipos de datos que implementan la interfaz System.Collections.Generic.IEnumerable\u0026lt;T\u0026gt;.\nstatic void WriteNames(params IEnumerable\u0026lt;string\u0026gt; names) =\u0026gt; Console.WriteLine(String.Join(\u0026#34;, \u0026#34;, names)); WriteNames(persons.Select(person =\u0026gt; person.FirstName)); WriteNames(from p in persons select p.FirstName); New lock type and semantics .NET 9 permitirá bloquear el acceso a recursos compartidos se pueda realizar de una manera más simple, eficiente y menos ambigua a través de la clase System.Threading.Lock.\nPara poder usar esta nueva clase en nuestras aplicaciones existentes, solo tendremos que sustituir private object myLock = new object(); por private System.Threading.Lock myLock = new System.Threading.Lock();. C# automáticamente generará las llamadas necesarias a la API para usar la nueva clase.\npublic class ClassLockTwo { private System.Threading.Lock myLock = new System.Threading.Lock(); public void MyMethod() { lock (myLock) { // Your code } } } New escape sequence - \\e. C# 13 introduce la secuencia de espace \\e. Esta secuencia equivale al código unicode \\u001b.\nConsole.WriteLine(\u0026#34;\\e[1mThis is a bold text\\e[0m\u0026#34;); Console.ReadLine(); Implicit indexer access in object initializers En C# podemos utilizar el operador ^ para acceder a un elemento de un Array desde el final del mismo. Con C# 13, podemos utilizarlo para inicializar elementos de un Array desde el final del mismo.\nvar countdown = new TimerRemaining(10) { Buffer = { [^1] = 0, [^10] = 9 } }; Console.WriteLine($\u0026#34;First: {countdown.Buffer.First()} Last: {countdown.Buffer.Last()}\u0026#34;); Console.ReadLine(); class TimerRemaining(int bufferSize) { public int[] Buffer { get; set; } = new int[bufferSize]; } ref struct A continuación podemos comprobar algunas novedades relacionadas con el tipo ref struct y C# 13.\nEnable ref locals and unsafe contexts in iterators and async methods Para versiones anteriores a C# 13, no es posible utilizar los tipos ref struct en metodos iteradores (yield return). En los métodos asincronos (async) tampoco se pueden declarar variables de este tipo ni pueden ser usadas en contextos inseguros. C# 13 nos permitirá hacer uso de este tipo en todos estos casos de uso.\nref struct ClassOne { public int Current =\u0026gt; 0; public bool MoveNext() =\u0026gt; false; public void Dispose() { } } class ClassTwo { public ClassOne GetEnumerator() =\u0026gt; new ClassOne(); async void M() { await Task.Yield(); using (new ClassOne()) { } lock (new System.Threading.Lock()) { } await Task.Yield(); } } Allow ref struct types as arguments for type parameters in generics. Versiones anteriores a C# 13 no permiten hacer uso del tipo ref structcomo parámetro generico de un método. A partir de esta versión ya es posible:\nT Identity\u0026lt;T\u0026gt;(T p) where T : allows ref struct =\u0026gt; p; var local = Identity(new User()); Console.ReadLine(); ref struct User { } Referencias https://devblogs.microsoft.com/dotnet/csharp-13-explore-preview-features/ https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-13 https://github.com/fjvela/csharp-13 ","permalink":"https://blog.javivela.dev/posts/2024/dotnet/csharp-13-net-9-preview-6/","summary":"Echamos un vistazo a las novedades de C# 13 (.NET 9 preview 6)\n- params collections\n- New lock type and semantics\n- New escape sequence - \\e.","title":"Novedades en C# 13 (.NET 9 preview 6)"},{"content":"Una de las mayores ventajas del uso de Kubernetes (K8S) para ejecutar nuestras aplicaciones es la escalabilidad, gracias a ella podemos gestionar los recursos de una manera más eficiente.\nExisten dos tipos de escalabilidad:, la escalabilidad horizontal, que aumenta el número de réplicas (Deployments o Stateful) de nuestra aplicación dependiendo del uso de memoria y CPU de la aplicación. Si el consumo de CPU y/o memoria aumenta, el número de réplicas aumentará, y si disminuye, disminuirá el número de réplicas de la aplicación. En Kubernetes, el HPA (Horizontal Pod Autoscaler) es el encargado de monitorizar y escalar horizontalmente las aplicaciones.\nLa escalabilidad vertical permite aumentar la memoria y CPU de las réplicas dinámicamente dependiendo del uso de memoria y CPU de la aplicación. El componente VPA (Vertical Pod Autoscaler) permite realizar estos cambios automáticamente en nuestro clúster Kubernetes.\nVertical Pod Autoscaler (VPA) Consta de tres componentes:\nRecommender: es el componente principal del VPA, cuya misión es generar recomendaciones de asignación de CPU y memoria basados en el histórico y consumo actual de CPU y memoria. Updater: decide qué Pods deben ser reiniciados basándose en los datos del recommender. Si se debe ajustar la CPU y memoria de un pod, intentará terminar su ejecución (revisando Pod Disruption Budget (PDB)) - evento EvictedByVPA . La actualización de la asignación de los recursos es realizada por el Admission Controller. Admission Controller: por cada Pod creado en el clúster, comprueba si debe realizar una actualización de los recursos asignados basándose en la configuración y recomendaciones del VPA (si la hay).\nObjecto VPA (Vertical Pod Autoscaler) Para poder definir el comportamiento del VPA para nuestros Pods es necesario crear un recurso de tipo VerticalPodAutoscaler:\napiVersion: \u0026#34;autoscaling.k8s.io/v1\u0026#34; kind: VerticalPodAutoscaler metadata: name: hamster-vpa spec: targetRef: apiVersion: \u0026#34;apps/v1\u0026#34; kind: Deployment name: hamster updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: \u0026#39;*\u0026#39; minAllowed: cpu: 100m memory: 50Mi maxAllowed: cpu: 1 memory: 500Mi controlledResources: [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;] targetRef: Indicaremos el recurso sobre el que el VPA actuará: kind: tipo de controlador (ej.: deployment o daemonset) que controla los Pods a monitorizar y actualizar los recursos asignados a los Pods name: nombre del recurso containerPolicies: containerName: Nombre del contenedor (podemos usar * para aplicar la configuración a todos los containers del pod) minAllowed: Los recursos mínimos que el VPA puede asignar al container (CPU y/o memoria) maxAllowed: Los recursos máximos que el VPA puede asignar al container (CPU y/o memoria) controlledResources: El tipo de recurso o recusos a monitorizar (CPU y/o memoria) updatePolicy: VPA ofrece cuatro modos para aplicar las recomendaciones de asignación de CPU y memoria: Off: El recommender calcula las recomendaciones de asignación de recursos pero el updater nunca reinicia los Pods para aplicar los cambios (modo \u0026ldquo;dry-run\u0026rdquo;) Initial: Solo se se asignan las recomendaciones cuando se crea el pod, no se realiza ningún cambio durante el ciclo de vida del mismo Recreate: El VPA puede asignar las recomendaciones cuando se crea el Pod y durante su ciclo de vida Auto: Actualmente equivale a la opción Recreate ya que es el unico metodo de actualización disponible (valor por defecto) Instalación A día de hoy, VPA no está incluido como un componente en Kubernetes por lo que es necesario instalarlo desde su repositorio.\n\u0026#x26a0;\u0026#xfe0f; Revisa las instrucciones de instalación, es posible que hayan cambiado.\nMinikube Desplegar los componentes del VPA es muy sencillo:\nClonaremos el repositorio https://github.com/kubernetes/autoscaler/tree/master Ejecutamos el comando ./hack/vpa-up.sh desde la carpeta vertical-pod-autoscaler Azure (Azure Kubernetes Service) Azure permite habilitar/deshabilitar Vertical Pod Autoscaler en un clúster AKS. Para ello, debemos registrar el \u0026ldquo;feature flag\u0026rdquo; AKS-VPAPreview en nuestra subscripción utilizando el siguiente comando: az feature register --namespace \u0026quot;Microsoft.ContainerService\u0026quot; --name \u0026quot;AKS-VPAPreview\u0026quot;.\nUna vez registrado, podemos utilizar Terraform, Bicep o az cli para desplegar automáticamente VPA en nuestro clúster.\nSi estás utilizando Terraform, asegúrate de usar al menos la versión 3.47 de terraform-provider-azurerm y agrega el siguiente bloque de código al recurso azurerm_kubernetes_cluster:\nworkload_autoscaler_profile { vertical_pod_autoscaler_enabled = true } Puedes utilizar el código de Terraform de este repositorio para desplegar un clúster AKS con el componente VPA desplegado. Para facilitar la prueba, el clúster es público.\n\u0026#x26a0;\u0026#xfe0f; ¡No utilices este código para desplegar un clúster en un entorno productivo! Recuerda borrarlo una vez finalizadas las pruebas para evitar problemas y/o costes innecesarios.\nSi utilizas Bicep, puedes encontrar cómo habilitarlo en el siguiente enlace: https://learn.microsoft.com/en-us/azure/templates/microsoft.containerservice/managedclusters?pivots=deployment-language-bicep#managedclusterworkloadautoscalerprofileverticalpodau\nTambién puedes utilizar la línea de comandos az, puedes consultar los comandos necesarios en el enlace: https://learn.microsoft.com/en-us/azure/aks/vertical-pod-autoscaler#deploy-upgrade-or-disable-vpa-on-a-cluster\nTen en cuenta que la versión desplegada por Azure es una versión del Vertical Pod Autoscaler modificada por Microsoft, su funcionamiento y rendimiento deberían ser mejores y optimizados para trabajar en un clúster AKS.\nComprobación de la instalación Los componentes se instalan en el namespace kube-system, comprobamos que los tres componentes están arrancados y funcionan sin problema (kubectl get pods -l 'app in( vpa-admission-controller, vpa-recommender, vpa-updater)' -n kube-system)):\nFuncionamiento El siguiente paso es ejecutar un ejemplo para poder revisar el funcionamiento del VPA en el clúster, en el propio repositorio podemos encontrar el siguiente ejemplo:\napiVersion: \u0026#34;autoscaling.k8s.io/v1\u0026#34; kind: VerticalPodAutoscaler metadata: name: hamster-vpa spec: targetRef: apiVersion: \u0026#34;apps/v1\u0026#34; kind: Deployment name: hamster updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: \u0026#39;*\u0026#39; minAllowed: cpu: 100m memory: 50Mi maxAllowed: cpu: 1 memory: 500Mi controlledResources: [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;] --- apiVersion: apps/v1 kind: Deployment metadata: name: hamster spec: selector: matchLabels: app: hamster replicas: 2 template: metadata: labels: app: hamster spec: securityContext: runAsNonRoot: true runAsUser: 65534 # nobody containers: - name: hamster image: registry.k8s.io/ubuntu-slim:0.1 resources: requests: cpu: 100m memory: 50Mi command: [\u0026#34;/bin/sh\u0026#34;] args: - \u0026#34;-c\u0026#34; - \u0026#34;while true; do timeout 0.5s yes \u0026gt;/dev/null; sleep 0.5s; done\u0026#34; En el ejemplo, podemos ver la definición de un objeto VPA, el cual actuará sobre el Deployment hamster y monitorizará tanto la memoria como la CPU consumida por los pods del deployment. Los recursos mínimos y máximos que podrá asignar son los siguientes:\nMínimo Máximo CPU 100m 1 Memoria 50Mi 500Mi Logs recommender Cada 1 minuto, el recommender comprueba las métricas y calcula si es necesario realizar una actualización de los recursos:\nI0515 16:59:00.167961 1 recommender.go:168] Recommender Run I0515 16:59:00.168058 1 cluster_feeder.go:355] Start selecting the vpaCRDs. I0515 16:59:00.168066 1 cluster_feeder.go:390] Fetched 1 VPAs. I0515 16:59:00.168093 1 cluster_feeder.go:400] Using selector app=hamster for VPA default/hamster-vpa I0515 16:59:00.174398 1 metrics_client.go:77] 11 podMetrics retrieved for all namespaces I0515 16:59:00.174452 1 cluster_feeder.go:478] ClusterSpec fed with #22 ContainerUsageSamples for #11 containers. Dropped #0 samples. I0515 16:59:00.174460 1 recommender.go:178] ClusterState is tracking 13 PodStates and 1 VPAs I0515 16:59:00.194835 1 checkpoint_writer.go:114] Saved VPA default/hamster-vpa checkpoint for hamster I0515 16:59:00.194867 1 cluster.go:362] Garbage collection of AggregateCollectionStates triggered I0515 16:59:00.194890 1 recommender.go:188] ClusterState is tracking 12 aggregated container states Cuando se genera una recomendación, se puede consultar en el objeto VPA. Para ello ejecutamos el comando kubectl get vpa -o=jsonpath='{.items[0].status}' | jq :\nLogs updater En el caso de que haya una recomendación por parte del recommender, el updater eliminará los Pods asociados al deployment (razón: EvictedByVPA ). Este proceso también se ejecuta cada 1 minuto.\nI0515 17:11:06.957414 1 api.go:92] Initial VPA synced successfully I0515 17:11:06.957601 1 reflector.go:221] Starting reflector *v1.Pod (1h0m0s) from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:289 I0515 17:11:06.957675 1 reflector.go:257] Listing and watching *v1.Pod from k8s.io/autoscaler/vertical-pod-autoscaler/pkg/updater/logic/updater.go:289 I0515 17:12:06.959731 1 update_priority_calculator.go:143] pod accepted for update default/hamster-65cd4dd797-mc9f5 with priority 8.870000000000001 I0515 17:12:06.959861 1 update_priority_calculator.go:143] pod accepted for update default/hamster-65cd4dd797-jqblb with priority 8.870000000000001 I0515 17:12:06.959890 1 updater.go:215] evicting pod hamster-65cd4dd797-mc9f5 I0515 17:12:06.976619 1 event.go:285] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;hamster-65cd4dd797-mc9f5\u0026#34;, UID:\u0026#34;d22d508d-26f4-4d89-b371-edd744018f57\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;118847\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;EvictedByVPA\u0026#39; Pod was evicted by VPA Updater to apply resource recommendation. I0515 17:13:06.951468 1 update_priority_calculator.go:143] pod accepted for update default/hamster-65cd4dd797-jqblb with priority 8.870000000000001 I0515 17:13:06.951508 1 update_priority_calculator.go:129] not updating a short-lived pod default/hamster-65cd4dd797-jxgtj, request within recommended range I0515 17:13:06.951520 1 updater.go:215] evicting pod hamster-65cd4dd797-jqblb I0515 17:13:06.969737 1 event.go:285] Event(v1.ObjectReference{Kind:\u0026#34;Pod\u0026#34;, Namespace:\u0026#34;default\u0026#34;, Name:\u0026#34;hamster-65cd4dd797-jqblb\u0026#34;, UID:\u0026#34;c7de721e-cc47-4906-8c15-10b4e4725b82\u0026#34;, APIVersion:\u0026#34;v1\u0026#34;, ResourceVersion:\u0026#34;118845\u0026#34;, FieldPath:\u0026#34;\u0026#34;}): type: \u0026#39;Normal\u0026#39; reason: \u0026#39;EvictedByVPA\u0026#39; Pod was evicted by VPA Updater to apply resource recommendation. I0515 17:14:06.949563 1 update_priority_calculator.go:129] not updating a short-lived pod default/hamster-65cd4dd797-jxgtj, request within recommended range I0515 17:14:06.949598 1 update_priority_calculator.go:129] not updating a short-lived pod default/hamster-65cd4dd797-qj6tg, request within recommended range Con el comando kubectl get event --field-selector reason=EvictedByVPA podemos comprobar los eventos de borrado de los Pods para poder asignarles los nuevos recursos:\nLogs Admission Controller Todas las solicitudes de creación de un nuevo Pod son enviadas a los admission controller desplegados en el clúster. En nuestro caso, comprobará si es necesario actualizar los recursos asignados al pod:\nI0515 17:11:27.569672 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 0 0001-01-01 00:00:00 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc000596d20 0xc00068cd80 []} {\u0026lt;nil\u0026gt; []}} I0515 17:11:27.598513 1 handler.go:79] Admitting pod {hamster-65cd4dd797-% hamster-65cd4dd797- default 0 0001-01-01 00:00:00 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[app:hamster pod-template-hash:65cd4dd797] map[] [{apps/v1 ReplicaSet hamster-65cd4dd797 bf57314a-36a6-48fd-a297-269d21f364a4 0xc000320d27 0xc000320d28}] [] [{kube-controller-manager Update v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:generateName\u0026#34;:{},\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:app\u0026#34;:{},\u0026#34;f:pod-template-hash\u0026#34;:{}},\u0026#34;f:ownerReferences\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;bf57314a-36a6-48fd-a297-269d21f364a4\\\u0026#34;}\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;f:containers\u0026#34;:{\u0026#34;k:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;hamster\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:args\u0026#34;:{},\u0026#34;f:command\u0026#34;:{},\u0026#34;f:image\u0026#34;:{},\u0026#34;f:imagePullPolicy\u0026#34;:{},\u0026#34;f:name\u0026#34;:{},\u0026#34;f:resources\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:requests\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:cpu\u0026#34;:{},\u0026#34;f:memory\u0026#34;:{}}},\u0026#34;f:terminationMessagePath\u0026#34;:{},\u0026#34;f:terminationMessagePolicy\u0026#34;:{}}},\u0026#34;f:dnsPolicy\u0026#34;:{},\u0026#34;f:enableServiceLinks\u0026#34;:{},\u0026#34;f:restartPolicy\u0026#34;:{},\u0026#34;f:schedulerName\u0026#34;:{},\u0026#34;f:securityContext\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:runAsNonRoot\u0026#34;:{},\u0026#34;f:runAsUser\u0026#34;:{}},\u0026#34;f:terminationGracePeriodSeconds\u0026#34;:{}}} }]} I0515 17:11:27.598984 1 matcher.go:68] Let\u0026#39;s choose from 1 configs for pod default/hamster-65cd4dd797-% I0515 17:11:27.600342 1 recommendation_provider.go:90] updating requirements for pod hamster-65cd4dd797-%. I0515 17:11:27.600588 1 recommendation_provider.go:57] no matching recommendation found for container hamster, skipping I0515 17:11:27.600671 1 server.go:112] Sending patches: [{add /metadata/annotations map[]} {add /metadata/annotations/vpaUpdates Pod resources updated by hamster-vpa: container 0: } {add /metadata/annotations/vpaObservedContainers hamster}] I0515 17:11:27.609234 1 handler.go:79] Admitting pod {hamster-65cd4dd797-% hamster-65cd4dd797- default 0 0001-01-01 00:00:00 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[app:hamster pod-template-hash:65cd4dd797] map[] [{apps/v1 ReplicaSet hamster-65cd4dd797 bf57314a-36a6-48fd-a297-269d21f364a4 0xc00080c927 0xc00080c928}] [] [{kube-controller-manager Update v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:generateName\u0026#34;:{},\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:app\u0026#34;:{},\u0026#34;f:pod-template-hash\u0026#34;:{}},\u0026#34;f:ownerReferences\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;bf57314a-36a6-48fd-a297-269d21f364a4\\\u0026#34;}\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;f:containers\u0026#34;:{\u0026#34;k:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;hamster\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:args\u0026#34;:{},\u0026#34;f:command\u0026#34;:{},\u0026#34;f:image\u0026#34;:{},\u0026#34;f:imagePullPolicy\u0026#34;:{},\u0026#34;f:name\u0026#34;:{},\u0026#34;f:resources\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:requests\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:cpu\u0026#34;:{},\u0026#34;f:memory\u0026#34;:{}}},\u0026#34;f:terminationMessagePath\u0026#34;:{},\u0026#34;f:terminationMessagePolicy\u0026#34;:{}}},\u0026#34;f:dnsPolicy\u0026#34;:{},\u0026#34;f:enableServiceLinks\u0026#34;:{},\u0026#34;f:restartPolicy\u0026#34;:{},\u0026#34;f:schedulerName\u0026#34;:{},\u0026#34;f:securityContext\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:runAsNonRoot\u0026#34;:{},\u0026#34;f:runAsUser\u0026#34;:{}},\u0026#34;f:terminationGracePeriodSeconds\u0026#34;:{}}} }]} I0515 17:11:27.609377 1 matcher.go:68] Let\u0026#39;s choose from 1 configs for pod default/hamster-65cd4dd797-% I0515 17:11:27.609391 1 recommendation_provider.go:90] updating requirements for pod hamster-65cd4dd797-%. I0515 17:11:27.609399 1 recommendation_provider.go:57] no matching recommendation found for container hamster, skipping I0515 17:11:27.609444 1 server.go:112] Sending patches: [{add /metadata/annotations map[]} {add /metadata/annotations/vpaUpdates Pod resources updated by hamster-vpa: container 0: } {add /metadata/annotations/vpaObservedContainers hamster}] I0515 17:12:00.170779 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 118818 1 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:12:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc000411b90 0xc00088c378 []} {0xc00088c450 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:12:06.982108 1 handler.go:79] Admitting pod {hamster-65cd4dd797-% hamster-65cd4dd797- default 0 0001-01-01 00:00:00 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[app:hamster pod-template-hash:65cd4dd797] map[] [{apps/v1 ReplicaSet hamster-65cd4dd797 bf57314a-36a6-48fd-a297-269d21f364a4 0xc0008a2157 0xc0008a2158}] [] [{kube-controller-manager Update v1 2023-05-15 17:12:06 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:generateName\u0026#34;:{},\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:app\u0026#34;:{},\u0026#34;f:pod-template-hash\u0026#34;:{}},\u0026#34;f:ownerReferences\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;bf57314a-36a6-48fd-a297-269d21f364a4\\\u0026#34;}\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;f:containers\u0026#34;:{\u0026#34;k:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;hamster\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:args\u0026#34;:{},\u0026#34;f:command\u0026#34;:{},\u0026#34;f:image\u0026#34;:{},\u0026#34;f:imagePullPolicy\u0026#34;:{},\u0026#34;f:name\u0026#34;:{},\u0026#34;f:resources\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:requests\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:cpu\u0026#34;:{},\u0026#34;f:memory\u0026#34;:{}}},\u0026#34;f:terminationMessagePath\u0026#34;:{},\u0026#34;f:terminationMessagePolicy\u0026#34;:{}}},\u0026#34;f:dnsPolicy\u0026#34;:{},\u0026#34;f:enableServiceLinks\u0026#34;:{},\u0026#34;f:restartPolicy\u0026#34;:{},\u0026#34;f:schedulerName\u0026#34;:{},\u0026#34;f:securityContext\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:runAsNonRoot\u0026#34;:{},\u0026#34;f:runAsUser\u0026#34;:{}},\u0026#34;f:terminationGracePeriodSeconds\u0026#34;:{}}} }]} I0515 17:12:06.982216 1 matcher.go:68] Let\u0026#39;s choose from 1 configs for pod default/hamster-65cd4dd797-% I0515 17:12:06.982226 1 recommendation_provider.go:90] updating requirements for pod hamster-65cd4dd797-%. I0515 17:12:06.983687 1 server.go:112] Sending patches: [{add /metadata/annotations map[]} {add /spec/containers/0/resources/requests/cpu 587m} {add /spec/containers/0/resources/requests/memory 262144k} {add /metadata/annotations/vpaUpdates Pod resources updated by hamster-vpa: container 0: cpu request, memory request} {add /metadata/annotations/vpaObservedContainers hamster}] I0515 17:13:00.163171 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 118878 2 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:13:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc0007366f0 0xc00080e708 []} {0xc00080e7e0 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:13:06.976062 1 handler.go:79] Admitting pod {hamster-65cd4dd797-% hamster-65cd4dd797- default 0 0001-01-01 00:00:00 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[app:hamster pod-template-hash:65cd4dd797] map[] [{apps/v1 ReplicaSet hamster-65cd4dd797 bf57314a-36a6-48fd-a297-269d21f364a4 0xc0008a2427 0xc0008a2428}] [] [{kube-controller-manager Update v1 2023-05-15 17:13:06 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:generateName\u0026#34;:{},\u0026#34;f:labels\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:app\u0026#34;:{},\u0026#34;f:pod-template-hash\u0026#34;:{}},\u0026#34;f:ownerReferences\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;k:{\\\u0026#34;uid\\\u0026#34;:\\\u0026#34;bf57314a-36a6-48fd-a297-269d21f364a4\\\u0026#34;}\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;f:containers\u0026#34;:{\u0026#34;k:{\\\u0026#34;name\\\u0026#34;:\\\u0026#34;hamster\\\u0026#34;}\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:args\u0026#34;:{},\u0026#34;f:command\u0026#34;:{},\u0026#34;f:image\u0026#34;:{},\u0026#34;f:imagePullPolicy\u0026#34;:{},\u0026#34;f:name\u0026#34;:{},\u0026#34;f:resources\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:requests\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:cpu\u0026#34;:{},\u0026#34;f:memory\u0026#34;:{}}},\u0026#34;f:terminationMessagePath\u0026#34;:{},\u0026#34;f:terminationMessagePolicy\u0026#34;:{}}},\u0026#34;f:dnsPolicy\u0026#34;:{},\u0026#34;f:enableServiceLinks\u0026#34;:{},\u0026#34;f:restartPolicy\u0026#34;:{},\u0026#34;f:schedulerName\u0026#34;:{},\u0026#34;f:securityContext\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:runAsNonRoot\u0026#34;:{},\u0026#34;f:runAsUser\u0026#34;:{}},\u0026#34;f:terminationGracePeriodSeconds\u0026#34;:{}}} }]} I0515 17:13:06.976162 1 matcher.go:68] Let\u0026#39;s choose from 1 configs for pod default/hamster-65cd4dd797-% I0515 17:13:06.976171 1 recommendation_provider.go:90] updating requirements for pod hamster-65cd4dd797-%. I0515 17:13:06.976209 1 server.go:112] Sending patches: [{add /metadata/annotations map[]} {add /spec/containers/0/resources/requests/cpu 587m} {add /spec/containers/0/resources/requests/memory 262144k} {add /metadata/annotations/vpaUpdates Pod resources updated by hamster-vpa: container 0: cpu request, memory request} {add /metadata/annotations/vpaObservedContainers hamster}] I0515 17:14:00.160455 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 118956 3 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:14:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc00042ff40 0xc0006a0510 []} {0xc0006a05e8 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:15:00.165045 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 119037 4 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:15:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc00037b320 0xc00088c750 []} {0xc00088c828 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:16:00.159714 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 119092 5 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:16:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc000597e10 0xc0005610b0 []} {0xc000561188 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:16:33.741189 1 reflector.go:559] k8s.io/autoscaler/vertical-pod-autoscaler/pkg/target/fetcher.go:94: Watch close - *v1.DaemonSet total 6 items received I0515 17:16:43.942408 1 reflector.go:559] k8s.io/client-go/informers/factory.go:134: Watch close - *v1.LimitRange total 7 items received I0515 17:17:00.163545 1 handler.go:91] Processing vpa: \u0026amp;{{VerticalPodAutoscaler autoscaling.k8s.io/v1} {hamster-vpa default 49f80082-26b4-4511-b096-b2e5a477b34d 119147 6 2023-05-15 17:11:27 +0000 UTC \u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; map[] map[kubectl.kubernetes.io/last-applied-configuration:{\u0026#34;apiVersion\u0026#34;:\u0026#34;autoscaling.k8s.io/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;VerticalPodAutoscaler\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;hamster-vpa\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;resourcePolicy\u0026#34;:{\u0026#34;containerPolicies\u0026#34;:[{\u0026#34;containerName\u0026#34;:\u0026#34;*\u0026#34;,\u0026#34;controlledResources\u0026#34;:[\u0026#34;cpu\u0026#34;,\u0026#34;memory\u0026#34;],\u0026#34;maxAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:1,\u0026#34;memory\u0026#34;:\u0026#34;500Mi\u0026#34;},\u0026#34;minAllowed\u0026#34;:{\u0026#34;cpu\u0026#34;:\u0026#34;100m\u0026#34;,\u0026#34;memory\u0026#34;:\u0026#34;50Mi\u0026#34;}}]},\u0026#34;targetRef\u0026#34;:{\u0026#34;apiVersion\u0026#34;:\u0026#34;apps/v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Deployment\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;hamster\u0026#34;},\u0026#34;updatePolicy\u0026#34;:{\u0026#34;updateMode\u0026#34;:\u0026#34;Auto\u0026#34;}}} ] [] [] [{kubectl-client-side-apply Update autoscaling.k8s.io/v1 2023-05-15 17:11:27 +0000 UTC FieldsV1 {\u0026#34;f:metadata\u0026#34;:{\u0026#34;f:annotations\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:kubectl.kubernetes.io/last-applied-configuration\u0026#34;:{}}},\u0026#34;f:spec\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:resourcePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerPolicies\u0026#34;:{}},\u0026#34;f:targetRef\u0026#34;:{},\u0026#34;f:updatePolicy\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:updateMode\u0026#34;:{}}}} } {recommender Update autoscaling.k8s.io/v1 2023-05-15 17:17:00 +0000 UTC FieldsV1 {\u0026#34;f:status\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:conditions\u0026#34;:{},\u0026#34;f:recommendation\u0026#34;:{\u0026#34;.\u0026#34;:{},\u0026#34;f:containerRecommendations\u0026#34;:{}}}} }]} {\u0026amp;CrossVersionObjectReference{Kind:Deployment,Name:hamster,APIVersion:apps/v1,} 0xc0004112c0 0xc000560390 []} {0xc000560468 [{RecommendationProvided True 2023-05-15 17:12:00 +0000 UTC }]}} I0515 17:17:00.846076 1 reflector.go:559] k8s.io/autoscaler/vertical-pod-autoscaler/pkg/target/fetcher.go:94: Watch close - *v1.Deployment total 20 items received Comprobamos las anotaciones de los Pods con el comando kubectl get pods -o=jsonpath='{.items[0].metadata.annotations}' | jq:\nComprobamos los nuevos recursos asignados con el comando kubectl get pods -o=jsonpath='{.items[0].spec.containers[0].resources}' | jq:\nLimitaciones Actualmente el estado del componente se encuentra en estado beta y tiene algunas limitaciones conocidas:\nCada vez que se actualizan los recursos, los Pods son recreados VPA no garantiza que una vez eliminados los Pods estos se puedan recrear (ej. no hay espacio suficiente en el nodepool) La escalabilidad vertical no debe utilizarse junto con la escalabilidad horizontal Antes de utilizar VPA, debemos comprobar los Admission controllers existentes Conclusión No en todos los casos de uso podemos hacer uso del Horizontal Pod Autoscaler (HPA) para escalar nuestras aplicaciones en clúster Kubernetes, aunque actualmente la escalabilidad vertical disponible en Kubernetes se encuentra en fase beta y tiene algunas limitaciones, creo que en el futuro se hará un mayor uso de ella, ya que combinada con una buena monitorización, una buena configuración del clúster autoscaler y un buen ajuste del tamaño de los nodepools del clúster podremos optimizar de manera muy efectiva los recursos hardware y a su vez los costes de los mismos.\nReferences https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler https://learn.microsoft.com/en-us/azure/aks/vertical-pod-autoscaler https://learn.microsoft.com/en-us/azure/aks/vertical-pod-autoscaler#register-the-aks-vpapreview-feature-flag ","permalink":"https://blog.javivela.dev/posts/2023/kubernetes/vertical-pod-autoscaler/","summary":"La \u003cstrong\u003eescalabilidad vertical\u003c/strong\u003e permite \u003cstrong\u003eaumentar la memoria y CPU\u003c/strong\u003e de las réplicas dinámicamente dependiendo del uso de memoria y CPU de la aplicación. El componente VPA (Vertical Pod Autoscaler) permite realizar estos cambios automáticamente en nuestro clúster Kubernetes.","title":"Kubernetes - Vertical Pod Autoscaler"},{"content":"Ya tenemos disponibles las primeras novedades que incluirá C# 12 en el lanzamiento de .NET 8. A continuación revisamos algunas de ellas.\nPuedes encontrar el código fuente de los ejemplos en repositorio: https://github.com/fjvela/csharp-12\nPrimary constructors for classes and structs Los Primary constructors nos permiten definir una lista de parámetros en la definición de la clase, en lugar de tener que crear un constructor por separado. De esta manera, se puede definir una clase en una sola línea de código.\nEn C# 9 permitió la utilización de Primary constructors para las estructuras de datos record. C# 12 extiende esta funcionalidad a todas las clases, permitiendonos definir constructures de manera más concisa a tráves de Primary constructors.\nDefinición Primary constructors en clases public abstract class Figure(double x, double y) { public double X { get; } = x; public double Y { get; } = y; } public class Rectangle(double x, double y, double width, double height) : Figure(x, y) { public double Width { get; } = width; public double Height { get; } = height; } public class Circle(double x, double y, double radius) : Figure(x, y) { public double Radius { get; } = radius; } Using directives for additional types Actualmente la directiva using no nos permite crear alias para cualquier tipo de dato como por ejemplo: tuplas o arrays. Con C# 12 podemos crear alias para cualquier tipo como (string, int) o int[];.\nDefinición alias para cualquier tipo de datos using Address = (string city, string postalCode); using PathOfPoints = int[]; using DatabaseInt = int?; using Measurement = (string units, int distance); using Person = (string name, int age); void Method(Measurement x) { Console.WriteLine($\u0026#34;Method! {x.units} {x.distance} \u0026#34;); } Console.WriteLine(\u0026#34;Hello, human!\u0026#34;); Person person = new Person(\u0026#34;Javier\u0026#34;, 22); Address address = new Address(\u0026#34;My home\u0026#34;, \u0026#34;ES-50105\u0026#34;); PathOfPoints points = new int[] { 2, 5, 5 }; Method(new Measurement(\u0026#34;meters\u0026#34;, 23)); Default values for lambda expressions C# 12 incluye nuevas mejoras en la utilización de expresiones lambda. Con C# 12 podemos definir valores por defecto en los parámetros de las expresiones lambda.\nDefinición valores por defecto var sum = (int x = 0, int y = 0) =\u0026gt; x + y; Console.WriteLine(sum()); // Output: 0 Console.WriteLine(sum(1)); // Output: 1 Console.WriteLine(sum(1, 2)); // Output: 3 var greet = (string name = \u0026#34;World\u0026#34;, int times = 1) =\u0026gt; { string greeting = $\u0026#34;Hello, {name}!\u0026#34;; if (times \u0026gt; 1) { greeting += $\u0026#34; ({times} times)\u0026#34;; } return greeting; }; Console.WriteLine(greet()); // Output: Hello, World! Console.WriteLine(greet(\u0026#34;Alice\u0026#34;)); // Output: Hello, Alice! Console.WriteLine(greet(\u0026#34;Bob\u0026#34;, 3)); // Output: Hello, Bob! (3 times) Referencias https://devblogs.microsoft.com/dotnet/check-out-csharp-12-preview/ https://learn.microsoft.com/en-gb/dotnet/csharp/language-reference/proposals/primary-constructors https://learn.microsoft.com/en-gb/dotnet/csharp/language-reference/proposals/using-alias-types https://learn.microsoft.com/en-gb/dotnet/csharp/language-reference/proposals/lambda-method-group-defaults https://github.com/fjvela/csharp-12 ","permalink":"https://blog.javivela.dev/posts/2023/dotnet/csharp-12-net-8-preview-3/","summary":"Echamos un vistazo a las novedades de C# 12 (.NET 8 preview 3)\n- Primary constructors for classes and structs\n- Using directives for additional types\n- Default values for lambda expressions","title":"Novedades en C# 12 (.NET 8 preview 3)"},{"content":"Hace unas semanas se lanzaba la versión 1.10 de Dapr (16 de febrero de 2023) e incluía una novedad muy interesante, pluggable components.\nPluggable components Dapr incluye una gran colección de componentes que podemos utilizar en nuestras aplicaciones, en el caso de que necesitaramos crear un componente privado seria algo complejo ya que deberiamos hacer un fork de los repositorios dapr y components-contrib y utilizando Go podríamos desarrollar nuestro componente a medida.\nDapr 1.10 incluye la posibilidad de crear e integrar nuestros propios componentes sin necesidad de modificar el código de Dapr a tráves de pluggable components (preview). Un componente pluggable es aquel que no está incluido en el runtime de Dapr.\nLa comunicación entre el componente y Dapr se realiza a tráves de gRPC y Unix Domain Sockets por lo que podemos utilizar cualquier lenguaje de programación que soporte gRPC para desarrollar nuestro componente.\nA día de hoy podemos implementar 3 tipos de componentes. Dependiendo del tipo de componente, tendremos que implementar diferentes métodos:\nState Store: Permiten leer, almacenar y consultar elementos clave / valor en los almacenamientos implementados Pub/sub: Permiten la comunicación a tráves de eventos Bindings (Input / Ouput): Permiten invocar servicios externos y recibir eventos externos Desarrollo Para desarrollar el componente vamos a usar .NET 7 ya que soporta gRPC. Dapr ofrece una plantilla para .NET en el siguiente repositorio https://github.com/dapr/samples/tree/master/pluggable-components-dotnet-template.\nEl tipo de componente que vamos a desarrollar será de tipo Binding (output) por lo que la interfaz a implementar será la siguiente:\n1 método para la inicialización del componente initialization 1 método para indicar que el componente está funcionando correctamente (liveness check -Ping-) 1 método para ejecutar la lógica del componente 1 método para indicar la lista de acciones disponibles en el componente Una vez actualizada la plantilla a .NET 7 y actualizadas las referencias del proyecto:\nRegistramos el gRPC service en la clase Program, en nuestro caso OutputBindingService En la clase Service implementamos nuestra clase OutputBindingService, al tratarse de un Outbinding debemos implementar los métodos: Init Invoke ListOperations Ping Implementación - Init public override Task\u0026lt;OutputBindingInitResponse\u0026gt; Init(OutputBindingInitRequest request, ServerCallContext context) { logger.LogTrace(\u0026#34;Init\u0026#34;); LogMetadata(request.Metadata.Properties); var result = new OutputBindingInitResponse(); return Task.FromResult(result); } Implementación - Invoke /// \u0026lt;summary\u0026gt; /// Invoke remote systems with optional payloads. /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;request\u0026#34;\u0026gt;The request received from the client.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;context\u0026#34;\u0026gt;The context of the server-side call handler being invoked.\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;The response to send back to the client (wrapped by a task).\u0026lt;/returns\u0026gt; public override Task\u0026lt;InvokeResponse\u0026gt; Invoke(InvokeRequest request, ServerCallContext context) { logger.LogTrace(\u0026#34;Invoke\u0026#34;); LogMetadata(request.Metadata); logger.LogTrace(request.Data.ToString()); var result = new InvokeResponse() { ContentType = \u0026#34;application/json\u0026#34;, Data = request.Data, }; return Task.FromResult(result); } Implementación - List operations /// ListOperations list system supported operations. /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;request\u0026#34;\u0026gt;The request received from the client.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;context\u0026#34;\u0026gt;The context of the server-side call handler being invoked.\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;The response to send back to the client (wrapped by a task).\u0026lt;/returns\u0026gt; public override Task\u0026lt;ListOperationsResponse\u0026gt; ListOperations(ListOperationsRequest request, ServerCallContext context) { logger.LogTrace(\u0026#34;List\u0026#34;); var operations = new RepeatedField\u0026lt;string\u0026gt; { \u0026#34;read\u0026#34;, \u0026#34;write\u0026#34; }; var result = new ListOperationsResponse(); result.Operations.AddRange(operations); return Task.FromResult(result); } Implementación - Ping /// \u0026lt;summary\u0026gt; /// Ping the OutputBinding. Used for liveness porpuses. /// \u0026lt;/summary\u0026gt; /// \u0026lt;param name=\u0026#34;request\u0026#34;\u0026gt;The request received from the client.\u0026lt;/param\u0026gt; /// \u0026lt;param name=\u0026#34;context\u0026#34;\u0026gt;The context of the server-side call handler being invoked.\u0026lt;/param\u0026gt; /// \u0026lt;returns\u0026gt;The response to send back to the client (wrapped by a task).\u0026lt;/returns\u0026gt; public override Task\u0026lt;PingResponse\u0026gt; Ping(PingRequest request, ServerCallContext context) { logger.LogTrace(\u0026#34;Ping\u0026#34;); return Task.FromResult(new PingResponse()); } Compilamos y generamos la imagen Docker correspondiente, es importante que cuando generemos la imagen añadamos un usuario non-root:\nRUN adduser -u 5678 --disabled-password --gecos \u0026#34;\u0026#34; appuser \u0026amp;\u0026amp; chown -R appuser /app USER appuser Despliegue del componente Desplegar el nuestro componente pluggable es muy sencillo, tras generar la imagen con nuestra aplicación tan solo debemos generar un deployment y añadir las siguientes anotaciones a nivel de pod para indicarle a Dapr que registre nuestro componente:\ndapr.io/pluggable-components: \u0026#34;dapr-pluggable-component\u0026#34; dapr.io/app-id: \u0026#34;my-app\u0026#34; dapr.io/enabled: \u0026#34;true\u0026#34; Automaticamente Dapr Sidecar Injector creará otro contenedor para ejecutar el servicio de Dapr (Sidecar pattern) y modificará el deployment para compartir la carpeta /tmp/dapr-components-sockets entre los dos contenedores a traves de un volumeMounts.\nComprobamos el correcto funcionamiento del componente ejecutando directamente las peticiones al contenedor Dapr de nuestro deployment. Para ello:\nUtilizamos port forward para poder conectarnos al contenedor Dapr de nuestro deployment: kubectl port-forward deploy/mydapr-component 3500:3500 Ejecutamos las peticiones a nuestro pluggable component: curl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; http://localhost:3500/v1.0/bindings/prod-mystore -d \u0026#39;{ \u0026#34;data\u0026#34;: 100, \u0026#34;operation\u0026#34;: \u0026#34;write\u0026#34; }\u0026#39; curl -X POST -H \u0026#39;Content-Type: application/json\u0026#39; http://localhost:3500/v1.0/bindings/prod-mystore -d \u0026#39;{ \u0026#34;data\u0026#34;: 200, \u0026#34;operation\u0026#34;: \u0026#34;read\u0026#34; }\u0026#39; Creo que esta funcionalidad facilita en gran medida el poder desarrollar e integrar componentes privados, actualmente el único incoveniente es que es necesario desplegar el componente varias veces en el caso de que haya varias aplicaciones que lo utilicen.\nReferences https://docs.dapr.io/operations/components/pluggable-components-registration/ https://blog.dapr.io/posts/2023/02/16/dapr-v1.10-is-now-available/ ","permalink":"https://blog.javivela.dev/posts/2023/kubernetes/dapr-pluggable-components/","summary":"Echamos un vistazo a cómo crear un Pluggable components en Dapr","title":"Novedades en Dapr - Pluggable components"},{"content":"En noviembre de 2022 se lanzó .NET 7 y desde ese momento tenemos disponibles todas las novedades que trae C# 11. A continuación revisamos algunas de ellas.\nPuedes encontrar el código fuente de los ejemplos en repositorio: https://github.com/fjvela/csharp-11\nRaw strings Hasta ahora la manera de poder definir cadenas de texto multilínea en C# es utilizar el prefijo @. Uno de los problemas que presenta es que si se indenta el texto la salida de este se verá afectada o si se utilizan comillas dobles es necesario escaparlas.\nC# 11 incluirá una nueva manera de definir cadenas de texto, para ello deberemos utilizar como mínimo tres comillas \u0026quot;\u0026quot;\u0026quot;. Esto nos facilitará poder indentar cadenas de texto en nuestro código y evitar tener que escapar las comillas. En el caso de utilizar una cadena de texto interpolada (string interpolation - \u0026ldquo;Hello, I\u0026rsquo;m {{ name }}\u0026rdquo;), deberemos utilizar como mínimo dos dólares $$.\nDefinición de cadenas de texto multilínea en C# var myString = @\u0026#34; \u0026lt;element attr=\u0026#34;\u0026#34;content\u0026#34;\u0026#34;/\u0026gt;\u0026#34;; Resultado:\n\u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; Definición de cadenas de texto multilíneas en C# 11 var myStringRaw = \u0026#34;\u0026#34;\u0026#34; \u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; \u0026#34;\u0026#34;\u0026#34;; Resultado:\n\u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; Definición de cadenas de texto interpoladas en C# 11 var myJSONRawInterpolated = $$\u0026#34;\u0026#34;\u0026#34; { \u0026#34;name\u0026#34;: \u0026#34;{{name}}\u0026#34; } \u0026#34;\u0026#34;\u0026#34;; Resultado:\n{ \u0026#34;name\u0026#34;: \u0026#34;Javi Vela\u0026#34; } Resultado binario decompilado A continuación puedes ver el código decompilado: Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#raw-string-literals List patterns Nos permitirá comparar un patrón con un array o list de elementos, por lo que podríamos definir un método con los siguientes patrones:\nint CheckSwitch(int[] values) =\u0026gt; values switch { [1, 2, .., 10] =\u0026gt; 1, [1, 2] =\u0026gt; 2, [1, _] =\u0026gt; 3, [1, ..] =\u0026gt; 4, [21] =\u0026gt; 5, [21, _, ..] =\u0026gt; 6, [33, _, 34, .., 44] =\u0026gt; 7, [_, ..] =\u0026gt; 50 }; Los resultados de la ejecución son:\nConsole.WriteLine(CheckSwitch(new[] { 1, 2, 10 })); // prints 1 Console.WriteLine(CheckSwitch(new[] { 1, 2, 7, 3, 3, 10 })); // prints 1 Console.WriteLine(CheckSwitch(new[] { 1, 2 })); // prints 2 Console.WriteLine(CheckSwitch(new[] { 1, 3 })); // prints 3 Console.WriteLine(CheckSwitch(new[] { 1, 3, 5 })); // prints 4 Console.WriteLine(CheckSwitch(new[] { 2, 5, 6, 7 })); // prints 50 Console.WriteLine(CheckSwitch(new[] { 21, 52, 63, 74, 5 })); // prints 5 Console.WriteLine(CheckSwitch(new[] { 21 })); // prints 6 Console.WriteLine(CheckSwitch(new[] { 33, 0, 34, 1, 2, 3, 44 })); // prints 7 También podemos utilizarlo con la sentencia if y obtener los valores del mismo:\nif (numbers is [var first, _, _]) { Console.WriteLine($\u0026#34;The first element of a three-item list is {first}.\u0026#34;); } if (numbers is [1, var second, _]) { Console.WriteLine($\u0026#34;The second element of a three-item list is {second}.\u0026#34;); } Resultado binario decompilado A continuación puedes ver el código decompilado: Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#list-patterns UTF-8 String literals Si tu aplicación necesita cadenas de texto en formato UTF-8 (ej: necesita comunicarse con otras a través de protocolos HTTP), el sufijo u8 puede ahorrarte unas líneas de código.\nAñadiendo el sufijo u8, automáticamente transforma la cadena de texto en un array de bytes en formato UTF-8.\nPor defecto para convertir una cadena de texto en un array de bytes en UTF-8, necesitas:\nbyte[] data = System.Text.Encoding.UTF8.GetBytes(\u0026#34;Javi Vela\u0026#34;); Gracias al sufijo \u0026lsquo;u8\u0026rsquo;, simplificamos la conversión:\nbyte[] data = \u0026#34;Javi Vela\u0026#34;u8.ToArray(); Resultado binario decompilado A continuación puedes ver el código decompilado: Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#utf-8-string-literals Newlines in string interpolation expressions Versiones anteriores C# 11 no permiten el uso de saltos de línea en cadenas de texto interpoladas (string interpolation - \u0026ldquo;Hello, I\u0026rsquo;m {{ name }}\u0026rdquo;).\nEn el caso de utilizarlas, el compilador da un error como podemos ver en la siguiente imagen.\nC# 11 nos permite añadir saldos de línea sin modificar el formato del texto:\nConsole.WriteLine($\u0026#34;Hello {name }!!, How are you?\u0026#34;); Resultado binario decompilado A continuación puedes ver el código decompilado: Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#newlines-in-string-interpolations Generic math support C# 11 incluye el soporte genérico de los datos matemáticos, permitiéndonos construir métodos genéricos como el siguiente:\nT AddAll\u0026lt;T\u0026gt;(T[] items) where T : INumber\u0026lt;T\u0026gt; { T result = T.Zero; foreach (var item in items) result += item; return result; } int[] numbers = new[] { 1, 5, 6, 9, 19}; Console.WriteLine(AddAll(numbers)); Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#generic-math-support Required members Existe un nuevo modificador required que podemos añadir a nuestras propiedades para indicar que los constructores deben inicializar esa propiedad, en caso contrario nuestra aplicación no compilara.\nEl siguiente código muestra la inicialización de una clase pero no se ha inicializado la propiedad Name en el constructor por lo que tendremos un error de compilación.\nvar user = new User { }; Console.WriteLine($\u0026#34;Name: {user.Name}\u0026#34;); class User { public required string Name { get; init; } } Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#required-members File-local types Se incluye en C# 11 un nuevo modificador de clases, file. Al indicar el modificador de acceso file estamos indicando que la visibilidad (scope) de la clase es dentro del mismo fichero.\nResultado binario decompilado A continuación puedes ver el código decompilado:\nLeer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#file-local-types Auto-default-structs Versiones anteriores a C# 11 obligan a inicializar todas las propiedades de un struct. C# 11 inicializará automáticamente todas las propiedades que no han sido inicializadas.\nEn la siguiente imagen podemos ver como automáticamente se ha inicializado la propiedad \u0026lsquo;Y\u0026rsquo; con el valor por defecto de su tipo, en este caso cero.\nResultado binario decompilado A continuación podemos ver la librería decompilada y como el compilador ha añadido el código necesario para inicializar la propiedad por nosotros: Leer más: https://learn.microsoft.com/en-us/dotnet/csharp/whats-new/csharp-11#auto-default-struct Referencias https://devblogs.microsoft.com/dotnet/early-peek-at-csharp-11-features/ https://github.com/dotnet/roslyn/blob/main/docs/Language%20Feature%20Status.md https://github.com/fjvela/csharp-11 ","permalink":"https://blog.javivela.dev/posts/2023/dotnet/csharp-11/","summary":"Echamos un vistazo a las novedades de C# 11","title":"Novedades en C# 11"},{"content":"Introducción En algunas ocasiones nuestras aplicaciones desplegadas en kubernetes necesitan información sensible y/o confidencial tales como una contraseña de una base de datos o un token para conectarse a otras aplicaciones.\nDesplegar y mantener esta información en nuestro cluster no es siempre trivial o sencilla y en determinados casos de uso es posible que el tipo de objecto Secret no sea el mas adecuado.\nA lo largo del post vamos a ver como instalar y configurar Azure Key Vault Provider for Secrets Store CSI Driver, este componente nos va a permitir leer información de un Azure Key Vault y proporcionarla a un Pod a través de un fichero o una variable de entorno.\nAzure Key Vault Provider for Secrets Store CSI Driver Container Store Driver (CSI) es una interfaz que implementan los cloud provider para que los orquestadores de contenedores (Kubernetes, Mesos, Nomad,\u0026hellip;) puedan hacer uso de sus sistemas de almacenamiento a través de una interfaz agnostica a la tecnologia subyacente.\nSecrets Store CSI Driver Provider Permite leer información sensible o confidencial (secretos, certificados,\u0026hellip;) de un origen (ej. Azure Key Vault, Amazon Secrets o Google Secret Manager\u0026hellip;) montarlos en un Pod utilizando la interfaz Container Store Driver (CSI) o crear automáticamente un secreto con esta información únicamente cuando exista algún Pod que requiera esta información.\nAzure Key Vault Provider Secret Store CSI Driver, nos permite leer información almacenada en un Azure Key Vault y montarla en un Pod utilizando la interfaz CSI. Puede utilizarse en cualquier orquestador de contenedores.\nInstalación Si no tienes desplegado un cluster de kubernetes en Azure puedes utilizar el código de terraform de este repositorio para desplegar uno. Para facilitar la prueba, el cluster es público - ¡No uses este código para desplegar un clúster en un entorno productivo! Recuerda borrarlo una vez finalizadas las pruebas para evitar problemas y/o costes innecesarios.\nPara seguir el post, deberás tener instaladas las siguientes herramientas:\nHelm3 Kubectl cli Az cli Para empezar con la instalación debemos decidir qué método de autenticación utilizará Azure Key Vault Provider for Secrets Store CSI Driver para leer los secretos del Azure Key Vault. Actualmente existen varios mecanismos:\nWorkload Identity Managed Identities (System-assigned and User-assigned) Service Principal Vamos a utilizar Service Principal. Dependiendo de nuestro caso de uso quizá no sea el más recomendable ya que deberemos crear un secreto para almacenar la información de autenticación, pero es el método de autenticación más rápido y sencillo de configurar.\nCrear service principal (SP) Creamos un service principal (SP) ejecutando el siguiente comando:\naz ad sp create-for-rbac -n sp-secrets-store-csi-driver-provider-azure Una vez creado el Service Principal creamos un secreto en nuestro clúster Kubernetes con los valores de las claves appId (usa el valor de la propiedad clientid) y password (usa el valor de la propiedad clientsecret) y etiquetamos el secreto con la etiqueta secrets-store-creds secrets-store.csi.k8s.io/used=true\nkubectl create secret generic secrets-store-creds --from-literal clientid=\u0026lt;AZURE_CLIENT_ID\u0026gt; --from-literal clientsecret=\u0026lt;AZURE_CLIENT_SECRET\u0026gt; -n myapp kubectl label secret secrets-store-creds secrets-store.csi.k8s.io/used=true -n myapp Crear Azure Key Vault Creamos el Azure Key Vault donde almacenaremos nuestros secretos y añadimos un secreto:\naz keyvault create --name \u0026#34;kv-secret-store-csi-001\u0026#34; --resource-group \u0026#34;rg-secret-store-westeu\u0026#34; --location \u0026#34;westeurope\u0026#34; az keyvault secret set --name \u0026#34;mysupersecret\u0026#34; --vault-name \u0026#34;kv-secret-store-csi-001\u0026#34; --value \u0026#34;MyVaultValueSecret\u0026#34; Asignamos permisos de lectura para que el service principal que hemos creado anteriormente pueda leer los secretos:\naz keyvault set-policy -n \u0026#34;kv-secret-store-csi-001\u0026#34; --secret-permissions list get --spn \u0026lt;SERVICE_PRINCIPAL_ID\u0026gt; Instalación Azure Key Vault Provider Secret Store CSI Driver Podemos instalar Azure Key Vault Provider Secret Store CSI Driver de varias maneras:\nHelm 3 Deployment yamls En el caso de que estemos utilizando un clúster AKS, podemos habilitar el addon azure-keyvault-secrets-provider utilizando az cli: https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver#enable-and-disable-autorotation (tiene soporte oficial) En nuestro caso vamos a utilizar Helm. Para ello, agregamos el repositorio secrets-store-csi-driver-provider-azure e instalamos el chart csi-secrets-store-provider-azure/csi-secrets-store-provider-azure en el namespace kube-system:\nhelm repo add csi-secrets-store-provider-azure https://azure.github.io/secrets-store-csi-driver-provider-azure/charts helm install csi csi-secrets-store-provider-azure/csi-secrets-store-provider-azure --namespace kube-system --set secrets-store-csi-driver.syncSecret.enabled=true El chart permite configurar los componentes desplegados, puedes comprobar todos los parámetros de configuración aquí.\nUna vez instalado, comprobamos que los componentes han arrancado correctamente:\nkubectl get pods -l app=csi-secrets-store-provider-azure -n kube-system Caso de uso: montar la información de un secreto a través de un fichero A continuación creamos un objeto de tipo SecretProviderClass. Este objeto nos permite configurar el Azure Key Vault del que vamos a leer. Puedes encontrar una descripción de todos los parámetros de configuración aquí.\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: azure-kv-secret-store-csi-mount-file spec: provider: azure parameters: keyvaultName: \u0026#34;kv-secret-store-csi-001\u0026#34; # the name of the KeyVault tenantId: \u0026#34;YOUR_TENANT_ID\u0026#34; # the tenant ID of the KeyVault objects: | array: - | objectName: mysupersecret objectAlias: mysupersecret # [OPTIONAL available for version \u0026gt; 0.0.4] object alias objectType: secret # object types: secret, key or cert. For Key Vault certificates, refer to https://azure.github.io/secrets-store-csi-driver-provider-azure/configurations/getting-certs-and-keys/ for the object type to use objectVersion: \u0026#34;\u0026#34; # [OPTIONAL] object versions, default to latest if empty filePermission: 0755 # [OPTIONAL] permission for secret file being mounted into the Pod, default is 0644 if not specified. El siguiente paso será crear un Pod y configurar un volumen para montar los secretos que necesitamos en un directorio:\nkind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-mount-file spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \u0026#34;/bin/sleep\u0026#34; - \u0026#34;10000\u0026#34; volumeMounts: - name: secrets-store-inline mountPath: \u0026#34;/mnt/secrets-store\u0026#34; readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \u0026#34;azure-kv-secret-store-csi-mount-file\u0026#34; nodePublishSecretRef: # Only required when using service principal mode name: secrets-store-creds # Only required when using service principal mode. The name of the Kubernetes secret that contains the service principal credentials to access keyvault. Ejecutamos el siguiente comando para comprobar el contenido del fichero montado automáticamente por el componente:\nkubectl exec busybox-secrets-store-mount-file -n myapp -- cat /mnt/secrets-store/mysupersecret Caso de uso: montar la información de un secreto a través de una variable de entorno El valor del parámetro syncSecret.enabled debe estar configurado a \u0026rsquo;true\u0026rsquo;\nLa configuración para crear un secreto automáticamente, es muy similar. Tan solo tenemos que agregar el bloque de configuración secretObjects:\nsecretObjects: # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects - secretName: secret-created-automatically # name of the Kubernetes secret object type: Opaque # type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls) data: - key: mysupersecret # data field to populate objectName: mysupersecretalias # this could be the object name or the object alias La configuración completa quedaria de la siguiente manera:\napiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: azure-kv-secret-store-csi-mount-env-var spec: provider: azure parameters: keyvaultName: \u0026#34;kv-secret-store-csi-001\u0026#34; # the name of the KeyVault tenantId: \u0026#34;328214bc-e88f-43fb-bad1-8f64ff51d823\u0026#34; # the tenant ID of the KeyVault objects: | array: - | objectName: mysupersecret objectAlias: mysupersecretalias # [OPTIONAL available for version \u0026gt; 0.0.4] object alias objectType: secret # object types: secret, key or cert. For Key Vault certificates, refer to https://azure.github.io/secrets-store-csi-driver-provider-azure/configurations/getting-certs-and-keys/ for the object type to use objectVersion: \u0026#34;\u0026#34; # [OPTIONAL] object versions, default to latest if empty secretObjects: # [OPTIONAL] SecretObjects defines the desired state of synced Kubernetes secret objects - secretName: secret-created-automatically # name of the Kubernetes secret object type: Opaque # type of Kubernetes secret object (for example, Opaque, kubernetes.io/tls) data: - key: mysupersecret # data field to populate objectName: mysupersecretalias # this could be the object name or the object alias Desplegamos un Pod configurando el secreto como una variable de entorno:\nkind: Pod apiVersion: v1 metadata: name: busybox-secrets-store-mount-env-var spec: containers: - name: busybox image: k8s.gcr.io/e2e-test-images/busybox:1.29 command: - \u0026#34;/bin/sleep\u0026#34; - \u0026#34;10000\u0026#34; env: - name: SECRET_CREDENTIALS valueFrom: secretKeyRef: name: secret-created-automatically key: mysupersecret volumeMounts: - name: secrets-store-inline mountPath: \u0026#34;/mnt/secrets-store\u0026#34; readOnly: true volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \u0026#34;azure-kv-secret-store-csi-mount-env-var\u0026#34; nodePublishSecretRef: # Only required when using service principal mode name: secrets-store-creds # Only required when using service principal mode. The name of the Kubernetes secret that contains the service principal credentials to access keyvault. Ejecutamos el siguiente comando para comprobar el contenido de la variable de entorno:\nkubectl exec busybox-secrets-store-mount-env-var -n myapp -- env | grep SECRET_CREDENTIALS Rotación de secretos Azure Key Vault Provider Secret Store CSI Driver puede actualizar el valor de un secreto en el caso de que haya sido actualizado en el Azure Key Vault. Actualmente es una funcionalidad en estado alpha, para poder hacer uso de ella debemos habilitarla a través del parámetro secrets-store-csi-driver.enableSecretRotation.\nEsta funcionalidad tiene algunas limitaciones como por ejemplo que no actualiza correctamente el valor de los secretos en Kubernetes (https://github.com/Azure/secrets-store-csi-driver-provider-azure/issues/1007).\nMétricas Azure Key Vault Provider for Secrets Store CSI Driver nos ofrece diferentes métricas que podemos integrar con Prometheus.\nPara Poder consultar las métricas del componente Azure Key Vault Provider ejecuta el siguiente comando:\nkubectl port-forward -n kube-system ds/csi-csi-secrets-store-provider-azure 8898:8898 \u0026amp; curl localhost:8898/metrics Para poder consultar las métricas del componente Secrets Store CSI Driver ejecuta el siguiente comando:\nkubectl port-forward -n kube-system ds/secrets-store-csi-driver 8080:8080 \u0026amp; curl http://localhost:8080/metrics Puedes encontrar una descripción completa de todas las metricas en el siguiente enlace: https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver#metrics\nLos nombres de los daemon set y puertos pueden variar dependiendo de los parámetros de configuración.\nReferencias https://github.com/container-storage-interface https://kubernetes-csi.github.io/docs/ https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/ https://github.com/aws/secrets-store-csi-driver-provider-aws https://github.com/GoogleCloudPlatform/secrets-store-csi-driver-provider-gcp https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver#metrics ","permalink":"https://blog.javivela.dev/posts/2022/kubernetes/azure-key-vault-secret-store-csi/","summary":"\u003ch2 id=\"introducción\"\u003eIntroducción\u003c/h2\u003e\n\u003cp\u003eEn algunas ocasiones nuestras aplicaciones desplegadas en kubernetes necesitan información sensible y/o confidencial tales como una contraseña de una base de datos o un token para conectarse a otras aplicaciones.\u003c/p\u003e\n\u003cp\u003eDesplegar y mantener esta información en nuestro cluster no es siempre trivial o sencilla y en determinados casos de uso es posible que el tipo de objecto \u003ca href=\"https://kubernetes.io/docs/concepts/configuration/secret/\"\u003e\u003cem\u003eSecret\u003c/em\u003e\u003c/a\u003e no sea el mas adecuado.\u003c/p\u003e\n\u003cp\u003eA lo largo del post vamos a ver como instalar y configurar \u003cstrong\u003eAzure Key Vault Provider for Secrets Store CSI Driver\u003c/strong\u003e, este componente nos va a permitir leer información de un \u003cstrong\u003eAzure Key Vault\u003c/strong\u003e y proporcionarla a un Pod a través de un fichero o una variable de entorno.\u003c/p\u003e","title":"Azure Key Vault Provider for Secrets Store CSI Driver, leyendo y utilizando información de un Azure Key Vault desde AKS"},{"content":"Introducción Al comenzar un nuevo proyecto es necesario instalar SDKs o otras dependencias necesarias para desarrollar y ejecutar el proyecto, al principio es muy fácil recordar todos los pasos necesarios pero con el paso del tiempo seguramente ya no estén tan claros los pasos a seguir o qué dependencias son necesarias. Los Readme son muy útiles pero no automatizan el proceso de instalación y seguramente no se hayan actualizado correctamente.\nOtro escenario bastante común en nuestro día a día es trabajar en varios proyectos a la vez y con diferentes SDKs y dependencias entre ellos.\n¿Te imaginas poder distribuir y mantener versionado un entorno de desarrollo al igual que versionas y distribuyes una aplicación? Si, es posible. Visual Studio Code y el plugin Visual Studio Code Dev Containers te permite crear y utilizar un contenedor docker para instalar los requisitos necesarios para desarrollar, ejecutar y depurar tu aplicación.\nDev Container La configuración del contendor de desarrollo se define en el fichero devcontainer.json dentro de la carpeta .devcontainer (no olvides el punto 😄) en el fichero puedes definir:\nImagen docker a utilizar, fichero docker a construir o fichero docker compose a utilizar Instalar herramientas Instalar extensiones de Visual Studio Code Definir puertos Definir argumentos y configuración por defecto Cuando el contenedor está arrancado, se monta la carpeta de tu proyecto (volume mount) y se instala Visual Studio Code Server en el contenedor con la configuración indicada en el fichero.\nA continuación vamos a configurar un contenedor de desarrollo para el proyecto terraform provider for confluent cloud, para poder desarrollar el proyecto es necesario (puedes encontrar aqui todos los requisitos):\nGo 1.18 (to build the provider plugin) Terraform \u0026gt;= 0.14 Docker para ejecutar los test de aceptación Recomendado utilizar Terraform version manager tfutils/tfenv Para comenzar partiremos del repositorio de ejemplo para proyectos go https://github.com/Microsoft/vscode-remote-try-go. Al abrir el fichero devcontainer.json podemos diferenciar tres secciones: build, runArgs y customizations.\nEl objeto build permite definir cómo se va a construir nuestra imagen de desarrollo, en el siguiente ejemplo definimos que se va a utilizar el fichero docker Dockerfile indicando también los argumentos necesarios para construir la imagen. En el caso de que la imagen no esté almacenada localmente, se construirá la imagen con la configuración indicada.\n\u0026#34;build\u0026#34;: { \u0026#34;dockerfile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, \u0026#34;args\u0026#34;: { \u0026#34;VARIANT\u0026#34;: \u0026#34;1.18-bullseye\u0026#34;, \u0026#34;NODE_VERSION\u0026#34;: \u0026#34;lts/*\u0026#34; } También podemos utilizar una imagen ya creada (propiedad image) almacenada en un docker image registry (Docker Hub, Azure Container Registry,\u0026hellip;), para utilizar un fichero docker compose debemos utilizar la propiedad dockerComposeFile\nrunArgs permite definir un array de argumentos que se utilizarán para ejecutar la imagen de desarrollo:\n\u0026#34;runArgs\u0026#34;: [ \u0026#34;--cap-add=SYS_PTRACE\u0026#34;, \u0026#34;--security-opt\u0026#34;, \u0026#34;seccomp=unconfined\u0026#34; ], customizations define la configuración de la instancia de Visual Studio Code, puedes encontrar las herramientas y servicios soportados aqui:\n\u0026#34;customizations\u0026#34;: { \u0026#34;vscode\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;go.toolsManagement.checkForUpdates\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;go.useLanguageServer\u0026#34;: true, \u0026#34;go.gopath\u0026#34;: \u0026#34;/go\u0026#34;, \u0026#34;go.goroot\u0026#34;: \u0026#34;/usr/local/go\u0026#34; }, \u0026#34;extensions\u0026#34;: [ \u0026#34;golang.Go\u0026#34; ] } } Una vez creado el fichero devcontainer.json, Visual Studio Code lo detectará cuando abrimos la carpeta del proyecto y nos preguntará si deseamos abrir nuestra carpeta usando el contendor. Una vez abierto, automáticamente comenzará a construir la imagen en caso de que fuera necesario:\nEjecutamos los siguientes comandos para compilar el módulo y comprobar si nuestro contenedor de desarrollo funciona correctamente:\nmake deps make build Para poder ejecutar los test de aceptación (make testacc) es necesario tener instalado docker. Al estar trabajando dentro de un contenedor necesitamos habilitar docker in docker, permite a nuestro contenedor que se conecte al socket de docker de nuestra máquina y de esta manera podremos crear contenedores desde nuestro contenedor de desarrollo.\nPodemos encontrar el script necesario en el siguiente enlace https://github.com/microsoft/vscode-dev-containers/tree/main/containers/docker-in-docker una vez integrado en nuestro proyecto podemos utilizar docker desde nuestro contenedor. Los contenedores se ejecutarán en la máquina host.\nPuedes encontrar todo la configuración necesaria en el siguiente enlace: https://github.com/fjvela/terraform-provider-confluent/commit/f12fc48d89cf3ceea77a2f7b8d7e791f59d255df\nDev Container Features El 15 de septiembre de 2022 se lanzó Custom Dev Container Features, esta funcionalidad simplifica enormemente la configuración de nuestros contenedores de desarrollo permitiéndonos habilitar y deshabilitar Features desde el archivo devcontainer.json sin necesidad de añadir o eliminar scripts a nuestro contenedor.\nPor ejemplo para habilitar la funcionalidad docker in docker tan solo es necesario habilitar la funcionalidad docker-in-docker, anteriormente hemos necesitado incluir y adaptar varios ficheros de configuración:\n\u0026#34;ghcr.io/devcontainers/features/docker-in-docker:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;moby\u0026#34;: true } Cada Dev Container Feature se distribuye como tarballs, cada uno de ellos contiene al menos dos ficheros:\ninstall.sh: Script de instalación, se añade como una nueva capa de la imagen y se ejecuta durante la construcción de la imagen devcontainer-feature.json: Contiene información sobre la Feature y contiene opciones que pueden pasarse como argumentos al script install.sh Finalmente con menos de 20 líneas podemos configurar nuestro proyecto para utilizar Dev Container Features y poder distribuir y versionar nuestro entorno de desarrollo:\n{ \u0026#34;name\u0026#34;: \u0026#34;Go\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;mcr.microsoft.com/devcontainers/base:ubuntu\u0026#34;, // Any generic, debian-based image. \u0026#34;features\u0026#34;: { \u0026#34;ghcr.io/devcontainers/features/go:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;1.18\u0026#34; }, \u0026#34;ghcr.io/devcontainers/features/docker-in-docker:1\u0026#34;: { \u0026#34;version\u0026#34;: \u0026#34;latest\u0026#34;, \u0026#34;moby\u0026#34;: true }, \u0026#34;ghcr.io/devcontainers/features/terraform:1\u0026#34;: {}, \u0026#34;ghcr.io/devcontainers/features/git:1\u0026#34;: {} }, // Uncomment to connect as a non-root user. More info: https://aka.ms/vscode-remote/containers/non-root. \u0026#34;remoteUser\u0026#34;: \u0026#34;vscode\u0026#34; } Puedes encontrar todo la configuración necesaria en el siguiente enlace: https://github.com/fjvela/terraform-provider-confluent/commit/b0cc3b1a2e60b67424901596da3c0770866dff51\nReferencias https://code.visualstudio.com/docs/devcontainers/tutorial https://code.visualstudio.com/docs/devcontainers/create-dev-container https://github.com/microsoft/vscode-dev-containers https://github.com/Microsoft/vscode-remote-try-go https://github.com/devcontainers https://containers.dev/features ","permalink":"https://blog.javivela.dev/posts/2022/devtools/desarrolla-en-un-contenedor/","summary":"En algunas ocasiones es algo tedioso instalar los SDKs o dependencias de un proyecto para poder trabajar con él. Visual Studio Code Dev Containers te permite definir y distribuir el entorno de desarrollo de tu proyecto.","title":"Mejora la experiencia de desarrollo, desarrolla dentro de un contenedor"},{"content":"Cuando desplegamos una aplicación en Kubernetes debemos informar de su estado al cluster de Kubernetes para que asi pueda saber si la aplicación está funcionando correctamente y así actuar en consecuencia (ej. reiniciar el Pod). Hasta la versión 1.18 Kubernetes disponía de dos probes:\nLiveness probe: Comprueba si la aplicación está funcionando correctamente. Readiness probe: Comprueba si la aplicación puede aceptar tráfico recibido desde un servicio. Para comprobar el estado de cada probe, kubertenes nos permite utilizar las siguientes acciones:\nexec: Ejecuta el comando indicado en el contenedor. Si el resultado del comando es 0, la aplicación está funcionando correctamente. HTTP: Ejecuta una llamada HTTP GET. Si el código de respuesta está entre 200 y 399, la aplicación está funcionando correctamente. TPC: Ejecuta una llamada TCP. Si acepta la conexión, la aplicación está funcionando correctamente. gRPC: (1.24 - beta) Ejecuta petición gRPC health. El resultado determina si la aplicación está funcionando correctamente. En cada acción podemos configurar los siguientes parámetros:\ninitialDelaySeconds: Tiempo de espera antes de la primera ejecución del probe (por defecto 0 segundos). periodSeconds: Tiempo de espera entre ejecuciones del probe (por defecto 10 segundos). timeoutSeconds: Tiempo máximo de espera para la ejecución del probe (por defecto 1 segundo). failureThreshold: Número de ejecuciones fallidas necesarias para que la aplicación se reinicie (por defecto 3). Para las aplicaciones que necesitan mucho tiempo para arrancar se puede configurar un valor alto para el parámetro initialDelaySeconds. Esto no es una buena práctica y en algunas ocasiones no funciona como se espera ya que kubelet no realiza ninguna comprobación hasta que no pasa el tiempo configurado en el parámetro initialDelaySeconds.\nLa versión 1.18 de Kubernetes nos permite configurar un nuevo tipo de probe: Startup probe. La configuración de la probe es idéntica a la configuración de las otras probes y permite comprobar si la aplicación ha arrancado correctamente. Una vez que la aplicación ha arrancado correctamente, kubelet comenzará a monitorizar el resto de probes (recuerda que aunque no las configures hay unos valores por defecto).\nEn el siguiente ejemplo, kubelet comprueba el estado de la probe Startup probecada dos segundos, en el caso de que falle diez veces reiniciaría el contenedor (tiempo total: veinte segundos). En caso de que la aplicación arranque en cinco segundos, kubelet comenzaria a monitorizar las otras probes.\napiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:latest startupProbe: httpGet: path: / port: 80 periodSeconds: 2 failureThreshold: 10 liveness: tcpSocket: port: 80 periodSeconds: 2 failureThreshold: 2 Referencias https://Kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes ","permalink":"https://blog.javivela.dev/posts/2022/kubernetes/startup-probe/","summary":"La probe \u003ccode\u003eStartup probe\u003c/code\u003e permite gestionar de una manera mas eficiente el arranque de una aplicación mejorando la disponibilidad de la misma.","title":"Kubernetes: Startup probe"},{"content":" Advertencia: Las nuevas funcionalidades de C# 11 se encuentran en modo preview y es posible que sufran cambios.\nYa podemos revisar alguna de la novedades que traerá C# 11, para ello deberás instalar .NET 6.0.200 y configurar la propiedad LangVersion con el valor Preview en los ficheros .csproj de tus proyectos:\n\u0026lt;Project Sdk=\u0026#34;Microsoft.NET.Sdk\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;OutputType\u0026gt;Exe\u0026lt;/OutputType\u0026gt; \u0026lt;TargetFramework\u0026gt;net6.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;ImplicitUsings\u0026gt;enable\u0026lt;/ImplicitUsings\u0026gt; \u0026lt;Nullable\u0026gt;enable\u0026lt;/Nullable\u0026gt; \u0026lt;LangVersion\u0026gt;preview\u0026lt;/LangVersion\u0026gt; \u0026lt;/PropertyGroup\u0026gt; \u0026lt;/Project\u0026gt; Puedes encontrar el código fuente de los ejemplos en el siguiente repositorio: https://github.com/fjvela/csharp-11\nRaw strings Hasta ahora la manera de poder definir cadenas de texto multilínea en C# es utilizar el prefijo @. Uno de los problemas que presenta es que si se identa el texto la salida del este se verá afectada o si se utilizan comillas es necesario escaparlas.\nC# 11 incluirá una nueva manera de definir cadenas de texto, para ello deberemos utilizar como mínimo tres comillas \u0026quot;\u0026quot;\u0026quot;. Esto nos facilitará poder identar cadenas de texto en nuestro código y evitar tener que escapar las comillas.\nDefinición de cadenas de texto multilínea en C# var myString = @\u0026#34; \u0026lt;element attr=\u0026#34;\u0026#34;content\u0026#34;\u0026#34;/\u0026gt;\u0026#34;; Resultado:\n\u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; Definición de cadenas de texto multilíneas en C# 11 var myStringRaw = \u0026#34;\u0026#34;\u0026#34; \u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; \u0026#34;\u0026#34;\u0026#34;; Resultado:\n\u0026lt;element attr=\u0026#34;content\u0026#34;/\u0026gt; Leer más: https://github.com/dotnet/csharplang/blob/main/proposals/raw-string-literal.md Simplified parameter null validation code (!!) A la hora de validar si un parámetro tiene un valor nulo podemos utilizar el siguiente código:\nvoid MyAwesomeMethod(string mystring) { if (mystring == null) throw new ArgumentNullException(nameof(mystring)); } C# 11 nos ofrecerá realizar esta comprobación de una manera más sencilla, tan solo tenemos que añadir !! como sufijo al nombre del parámetro:\nvoid MyAwesomeMethod(string mystring!!) { } Leer más: https://github.com/dotnet/csharplang/blob/main/proposals/param-nullchecking.md List pattens Nos permitirá comparar un patrón con un array o list de elementos, por lo que podríamos definir un método con los siguientes patrones:\nint CheckSwitch(int[] values) =\u0026gt; values switch { [1, 2, .., 10] =\u0026gt; 1, [1, 2] =\u0026gt; 2, [1, _] =\u0026gt; 3, [1, ..] =\u0026gt; 4, [21] =\u0026gt; 5, [21, _, ..] =\u0026gt; 6, [33, _, 34, .., 44] =\u0026gt; 7, [_, ..] =\u0026gt; 50 }; El resultado de la ejecución sería:\nConsole.WriteLine(CheckSwitch(new[] { 1, 2, 10 })); // prints 1 Console.WriteLine(CheckSwitch(new[] { 1, 2, 7, 3, 3, 10 })); // prints 1 Console.WriteLine(CheckSwitch(new[] { 1, 2 })); // prints 2 Console.WriteLine(CheckSwitch(new[] { 1, 3 })); // prints 3 Console.WriteLine(CheckSwitch(new[] { 1, 3, 5 })); // prints 4 Console.WriteLine(CheckSwitch(new[] { 2, 5, 6, 7 })); // prints 50 Console.WriteLine(CheckSwitch(new[] { 21, 52, 63, 74, 5 })); // prints 5 Console.WriteLine(CheckSwitch(new[] { 21 })); // prints 6 Console.WriteLine(CheckSwitch(new[] { 33, 0, 34, 1, 2, 3, 44 })); // prints 7 Leer más: https://github.com/dotnet/csharplang/blob/main/proposals/list-patterns.md Referencias https://devblogs.microsoft.com/dotnet/early-peek-at-csharp-11-features/ https://github.com/dotnet/roslyn/blob/main/docs/Language%20Feature%20Status.md ","permalink":"https://blog.javivela.dev/posts/2022/dotnet/csharp-11-parte-1/","summary":"Echamos un vistazo a las novedades de C# 11: Raw strings, Simplified parameter null validation code (!!) y List pattens","title":"Novedades en C# 11 (preview)"},{"content":" Advertencia: Antes de aplicar cualquier política o proceso de borrado de imágenes, es altamente recomendable que revises las políticas y procesos en modo “dry run” y revises los resultados.\nNos guste o no nos guste el almacenamiento no es infinito, el límite puede ser físico o económico y con la parte económica seguro que más de uno nos hemos llevado una sorpresa revisando la factura de nuestro cloud provider.\nLa factura de consumo adicional del servidor en #AWS Óleo sobre lienzo. pic.twitter.com/shve1YsGZh\n\u0026mdash; Héctor Arley Díaz (@hectorarley) September 11, 2021 Uno de los elementos a los que no solemos prestar atención es al almacenamiento de nuestros container registries, pudiendo llegar a almacenar cientos o miles de imágenes generadas en nuestros procesos de CI y que pasados unos días normalmente ya no son necesarias. A continuación vamos a mostrar como podemos limpiar nuestro Amazon Elastic Container Registry (Amazon ECR) o Microsoft Container Registry (Azure ACR). Amazon Elastic Container Registry (Amazon ECR) Amazon nos permite configurar políticas de ciclo de vida (Lifecycle policies) para cada uno de nuestros repositorios pudiendo definir varias reglas.\nCada política puede ser configurada manualmente (opción NO recomendada) o crearla de manera programática. Mi recomendación es que la creéis dentro de un paso en vuestro proceso de CI, si en el futuro necesitáis realizar un cambio de una política o regla en varios repositorios será una tarea rápida, fácil e indolora.\nLa política se define a través de un JSON pudiendo definir varias reglas con diferentes filtros:\nFecha push Numero de imágenes Estado Tag (expresión regular) { \u0026#34;rules\u0026#34;: [ { \u0026#34;rulePriority\u0026#34;: 1, \u0026#34;description\u0026#34;: \u0026#34;Expire images older than 14 days\u0026#34;, \u0026#34;selection\u0026#34;: { \u0026#34;tagStatus\u0026#34;: \u0026#34;untagged\u0026#34;, \u0026#34;countType\u0026#34;: \u0026#34;sinceImagePushed\u0026#34;, \u0026#34;countUnit\u0026#34;: \u0026#34;days\u0026#34;, \u0026#34;countNumber\u0026#34;: 14 }, \u0026#34;action\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;expire\u0026#34; } } ] } Una vez tengáis la política definida (se puede validar desde la consola de AWS https://docs.aws.amazon.com/AmazonECR/latest/userguide/lpp_creation.html podéis configurarla a través del comando: aws ecr put-lifecycle-policy --repository-name ${REPOSITORY} --lifecycle-policy-text file://ecr_lifecycle_policy.json Azure Container Registry (Azure ACR) En Azure no existen las políticas de ciclo de vida (Lifecycle policies) para eliminar imágenes. Para ello debemos hacer uso de Azure cli y el comando acr purge.\nEl comando acr purge acepta dos parámetros:\n\u0026ndash;filter: Definimos a través de una expresión regular que imágenes hay que eliminar \u0026ndash;ago: elimina las imágenes con una antigüedad mayor a la indicada (ej. \u0026ndash;ago 1d, elimina las imágenes con una antiguedad mayor a 1d) Para ejecutar el comando podemos utilizar ACR Tasks (tiene un coste por tiempo de ejecución), este servicio nos permite ejecutar tareas mantenimiento sobre nuestro ACR de manera puntual o programada.\nAl igual que con Amazon, mi recomendación es que ejecutéis el borrado dentro de vuestro proceso CI.\nPURGE_CMD=\u0026#34;acr purge \\ --filter \u0026#34;${REPOSITORY}:^mytag-\u0026#34; \\ --filter \u0026#34;${REPOSITORY}:^myothertag-\u0026#34; \\ --untagged \\ --ago 15d\u0026#34; az acr run \\ --cmd \u0026#34;$PURGE_CMD\u0026#34; \\ --registry \u0026#34;$REGISTRY\u0026#34; \\ --timeout 300 \\ /dev/null Referencias https://docs.aws.amazon.com/AmazonECR/latest/userguide/LifecyclePolicies.html https://docs.microsoft.com/azure/container-registry/container-registry-tasks-overview https://docs.microsoft.com/azure/container-registry/container-registry-auto-purge\n","permalink":"https://blog.javivela.dev/posts/2022/cicd/eliminando-imagenes-container-registries/","summary":"Nos guste o no nos guste \u003cstrong\u003eel almacenamiento no es infinito\u003c/strong\u003e, el límite puede ser físico o económico y con la parte económica seguro que más de uno nos hemos llevado una sorpresa revisando la factura de nuestro cloud provider.","title":"Eliminando imágenes en nuestros container registries"},{"content":"Todos nos hemos enfrentado en alguna ocasión a depurar una aplicación que se ejecuta en un contenedor en un clúster Kubernetes. Normalmente (y siguiendo buenas prácticas) estos contenedores utilizan imágenes distroless (solo continen la aplicación y sus dependencias, no contienen ninguna shell u otros programas). Algunas de las ventajas de utilizar este tipo de imágenes son:\nMenor tamaño de imagen Menor vector de ataque Menor tiempo de descarga de la imagen Menor número de dependencias Una desventaja es que en caso de necesitar depurar algún comportamiento extraño de la aplicación no sería posible ya que no existe una shell ni ninguna otra herramienta que nos permita depurar la aplicación.\nContenedores efímeros Desde la versión 1.16 Kubernetes nos permite utilizar contenedores efímeros (en la versión 1.23 todavía están en estado beta), estos contenedores se añaden al pod que pertenece el contenedor pudiendo compartir los procesos del resto de contenedores que componen el pod.\nLos contenedores efímeros tienen unas características especiales:\nNo se pueden reiniciar No se pueden definir que recursos pueden utilizar (CPU y memoria) No se permiten abrir un puerto No se permite configurar ningún probe: Liveness, Readiness y Startup Utilizando el comando kubectl debug podemos crear contenedores efímeros (es necesario que el clúster permita el uso de EphemeralContainers):\nCrea un contenedor efímero y abre una consola remota en el nuevo contenedor: kubectl debug mypod -it --image=busybox Crea una copia del pod mypod en un nuevo pod (my-debugger), crea un contenedor efímero y abre una consola remota: kubectl debug mypod -it --image=busybox --copy-to=my-debugger Crea una copia del pod mypod en un nuevo pod y sustituye la imagen kubectl debug mypod -it --copy-to=my-debugger --image=debian --set-image=app=app:debug,sidecar=sidecar:debug Pruebas Vamos a realizar varias pruebas con una aplicación Node.JS y una aplicación .NET. El código podéis encontrarlo en el siguiente repositorio: https://github.com/fjvela/poc-k8s/tree/main/ephemeral-containers\nVamos a desplegar las aplicaciones utilizando minikube y Kubernetes 1.23.1, para ello ejecutamos: minikube start --kubernetes-version=v1.23.1 (si utilizas una versión anterior deberás incluir el parámetro --feature-gates=EphemeralContainers=true).\nNode.JS Desplegamos la aplicación Node.JS: kubectl apply -f .\\nodejs\\ -n demo desplegará un Service de tipo cluster IP y un pod controlado a tráves de un deployment Abrimos un puerto al servicio para realizar varias peticiones, cada petición crea un archivo en la ruta /app/requests : kubectl port-forward svc/nodejsapp-svc 8000:8000 -n demo Intentamos consultar los ficheros que ha creado en cada una de las peticiones del paso anterior kubectl exec nodejsapps-766c6db685-bxw8k -it --container nodejsapp -n demo -- ls -la /app/requests. Al estar utilizando una imagen distroless no nos es posible poder acceder al sistema de archivos Creamos un contenedor efímero: kubectl debug -it nodejsapps-766c6db685-bxw8k --image=busybox -n demo: No podemos ver los procesos del contenedor creado previamente ps aux No podemos acceder al sistema de archivos del contenedor previamente ls -la /proc Podemos realizar pruebas de red ping google.com Creamos un contenedor efímero, copiando el pod existente en uno nuevo: kubectl debug -it nodejsapps-766c6db685-bxw8k --image=busybox --share-processes --copy-to=mynewpod -n demo Podemos ver los procesos del contenedor creado previamente ps aux Podemos acceder al sistema de archivos del contenedor previemte ls -la /proc/8/root/app/ Podemos realizar pruebas de red ping google.com Cuando se realiza la copia del contenedor, no se copian los archivos creados anteriormente El nuevo pod no está controlado por el deployment creado previamente por lo que las peticiones realizadas a traves del servicio no son dirigidas hacia el nuevo pod kubectl describe pod mynewpod -n demo .NET Desplegamos la aplicación .NET: kubectl apply -f .\\dotnet\\ -n demo desplegará un pod controlado por un deployment Creamos un contenedor efímero: kubectl debug -it theweather-99bb677cf-lx5h9 --image=fjvela/dotnet-debug-tools:6.0 --share-processes --copy-to=mynewpod -n demo: Podemos ver los procesos del contenedor creado previamente ps aux 3.- Ejecutamos el comando /tools dotnet-trace ps. Lamentablemente dotnet-trace no detecta el proceso por lo que no podemos utilizar las herramientas de depuración de .NET con contenedores efímeros. En el momento de escribir el articulo existe un petición abierta solicitando para dar soporte a esta funcionalidad: https://github.com/dotnet/diagnostics/issues/810 Referencias https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/ https://github.com/kubernetes/enhancements/issues/277 https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/ https://github.com/dotnet/diagnostics/issues/810 ","permalink":"https://blog.javivela.dev/posts/2022/kubernetes/contenedores-efimeros/","summary":"Todos nos hemos enfrentado en alguna ocasión a depurar una aplicación que se ejecuta en un contenedor en un clúster Kubernetes. Normalmente (y siguiendo buenas prácticas) estos contenedores utilizan imágenes \u003ca href=\"https://github.com/GoogleContainerTools/distroless\"\u003edistroless\u003c/a\u003e (solo continen la aplicación y sus dependencias, no contienen ninguna shell u otros programas)","title":"Kubernetes: depurando aplicaciones con contenedores efímeros"},{"content":"En algunas ocasiones hemos necesitado obtener la información del tráfico de entrada y salida (HTTP request / HTTP response) de nuestras aplicaciones ASP.NET core. Hasta la versión 6 de .NET era necesario escribir nuestro propio middleware. En esta versión, Microsoft ha incluido un middleware para poder loguear la siguiente información:\nHTTP request HTTP response Headers Common properties Body ¡Advertencia! El middleware HTTP logging puede afectar al rendimiento de la aplicación, también debes tener en cuenta que se puede mostrar información sensible de tu aplicación.\n¡Advertencia! Cuando se configura el middleware para loguear todas las cabederas de respuesta no se realiza de manera correcta, el bug ya había sido reportado y puedes comprobar el estado en el siguiente enlace: https://github.com/dotnet/aspnetcore/issues/36920\n¿Como habilitar HTTP logging? El método UseHttpLogging de la extensión HttpLoggingBuilderExtensions permite habilitar el middleware. Si necesitas que el formato de los logs tengan formato W3C utiliza el método UseW3CLogging.\npublic void Configure(IApplicationBuilder app, IWebHostEnvironment env) { app.UseHttpLogging(); app.UseRouting(); app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapGet(\u0026#34;/\u0026#34;, async context =\u0026gt; { await context.Response.WriteAsync(\u0026#34;Hello World!\u0026#34;); }); }); } Si utilizas minimal APIs\nvar builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); var app = builder.Build(); app.UseHttpLogging(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); Modifica el nivel de log Microsoft.AspNetCore.HttpLogging en el fichero de configuración appsettings.json.\n{ \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft.AspNetCore\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft.AspNetCore.HttpLogging\u0026#34;: \u0026#34;Information\u0026#34;, } }, \u0026#34;AllowedHosts\u0026#34;: \u0026#34;*\u0026#34; } Por defecto mostrará la siguiente información:\ninfo: Microsoft.AspNetCore.HttpLogging.HttpLoggingMiddleware[1] Request: Protocol: HTTP/2 Method: GET Scheme: https PathBase: Path: /WeatherForecast Accept: text/plain Host: localhost:7126 User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:94.0) Gecko/20100101 Firefox/94.0 :method: [Redacted] Accept-Encoding: gzip, deflate, br Accept-Language: en,es-ES;q=0.8,es;q=0.5,en-US;q=0.3 Cookie: [Redacted] Referer: [Redacted] TE: trailers DNT: 1 sec-fetch-dest: [Redacted] sec-fetch-mode: [Redacted] sec-fetch-site: [Redacted] Opciones de configuración personalizadas Puedes realizar una configuración personalizada utilizando el método AddHttpLogging:\npublic void ConfigureServices(IServiceCollection services) { services.AddHttpLogging(logging =\u0026gt; { // Customize HTTP logging here. logging.LoggingFields = HttpLoggingFields.All; logging.RequestHeaders.Add(\u0026#34;My-Request-Header\u0026#34;); logging.ResponseHeaders.Add(\u0026#34;My-Response-Header\u0026#34;); logging.MediaTypeOptions.AddText(\u0026#34;application/javascript\u0026#34;); logging.RequestBodyLogLimit = 4096; logging.ResponseBodyLogLimit = 4096; }); } Si utilizas minimal APIs\nvar builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); builder.Services.AddHttpLogging(options =\u0026gt; { // Customize HTTP logging here. options.LoggingFields = Microsoft.AspNetCore.HttpLogging.HttpLoggingFields.All; options.RequestHeaders.Add(\u0026#34;My-Request-Header\u0026#34;); options.ResponseHeaders.Add(\u0026#34;My-Response-Header\u0026#34;); options.MediaTypeOptions.AddText(\u0026#34;application/javascript\u0026#34;); options.RequestBodyLogLimit = 4096; options.ResponseBodyLogLimit = 4096; }); var app = builder.Build(); app.UseHttpLogging(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseHttpsRedirection(); app.UseAuthorization(); app.MapControllers(); app.Run(); LoggingFields: Te permite configurar que elementos se van a loguear. Por ejemplo el flag LoggingFields.RequestProtocol solo mostrará el protocolo utilizado en la petición. RequestHeaders: Logueará aquellos request headers que estén incluidos en la lista. ResponseHeaders: Logueará aquellas response headers que estén incluidos en la lista. RequestBodyLogLimit: Tamaño máximo del body a loguear de la petición. Valor por defecto: 32 KB. RequestBodyLogLimit: Tamaño máximo del body a loguear de la respuesta. Valor por defecto: 32 KB. Rendimiento A tráves de una pequeña prueba de carga con artillery podemos comparar el rendimiento de nuestra aplicación con y sin el middleware HttpLogging. A continuación podemos comprobar los resultados.\nSin middleware -------------------------------- Summary report -------------------------------- vusers.created_by_name.Get weather forecast: ................ 7754 vusers.created.total: ....................................... 7754 vusers.completed: ........................................... 7754 vusers.session_length: min: ...................................................... 3.7 max: ...................................................... 78.5 median: ................................................... 8.2 p95: ...................................................... 21.1 p99: ...................................................... 32.1 http.request_rate: .......................................... 76/sec http.requests: .............................................. 15508 http.codes.307: ............................................. 7754 http.responses: ............................................. 7754 http.response_time: min: ...................................................... 0 max: ...................................................... 67 median: ................................................... 3 p95: ...................................................... 15 p99: ...................................................... 25.8 expect.ok: .................................................. 7754 expect.ok.statusCode: ....................................... 7754 errors.UNABLE_TO_VERIFY_LEAF_SIGNATURE: ..................... 7754 Con middleware -------------------------------- Summary report -------------------------------- vusers.created_by_name.Get weather forecast: ................ 3166 vusers.created.total: ....................................... 3166 vusers.completed: ........................................... 3164 vusers.session_length: min: ...................................................... 7.9 max: ...................................................... 2308.1 median: ................................................... 43.4 p95: ...................................................... 1064.4 p99: ...................................................... 1790.4 http.request_rate: .......................................... 36/sec http.requests: .............................................. 6330 http.codes.307: ............................................. 3164 http.responses: ............................................. 3164 http.response_time: min: ...................................................... 4 max: ...................................................... 2294 median: ................................................... 37.7 p95: ...................................................... 1064.4 p99: ...................................................... 1790.4 expect.ok: .................................................. 3164 expect.ok.statusCode: ....................................... 3164 errors.UNABLE_TO_VERIFY_LEAF_SIGNATURE: ..................... 3164 Referencias https://devblogs.microsoft.com/aspnet/asp-net-core-updates-in-net-6-preview-4/?WT.mc_id=DT-MVP-5004074#http-logging-middleware https://docs.microsoft.com/en-us/aspnet/core/fundamentals/http-logging/?view=aspnetcore-6.0 https://github.com/fjvela/netcore6-examples/tree/main/HttpLoggingMiddlewareSample ","permalink":"https://blog.javivela.dev/posts/2021/net-core-6/es/asp.net-core-6-httploggin-logrequests-logresponses/","summary":"En algunas ocasiones hemos necesitado obtener las información del tráfico de entrada y salida (HTTP request / HTTP response) de nuestras aplicaciones ASP.NET core. Hasta la versión 6 de .NET era necesario escribir nuestro propio middleware. En esta versión, Microsoft ha incluido un middleware para poder loguear esta información.","title":"ASP.NET Core 6 - HttpLogging (log request/responses)"}]